[
  {
    "objectID": "common.base_auto.html",
    "href": "common.base_auto.html",
    "title": "Hyperparameter Optimization",
    "section": "",
    "text": "Figure 1. Example of dataset split (left), validation (yellow) and test (orange). The hyperparameter optimization guiding signal is obtained from the validation set.\n\n\n\n\nBaseAuto\n\n BaseAuto (cls_model, h, config,\n           search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n           object at 0x7ff85d6dd670>, num_samples=10, cpus=2, gpus=0,\n           refit_with_val=False, verbose=False)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs.\n\n\n\nBaseAuto.fit\n\n BaseAuto.fit (dataset, val_size=0, test_size=0)\n\nBaseAuto.fit\nPerform the hyperparameter optimization as specified by the BaseAuto configuration dictionary config.\nThe optimization is performed on the TimeSeriesDataset using temporal cross validation with the validation set that sequentially precedes the test set.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset see details here val_size: int, size of temporal validation set (needs to be bigger than 0). test_size: int, size of temporal test set (default 0).\nReturns: self: fitted instance of BaseAuto with best hyperparameters and results.\n\n\n\nBaseAuto.predict\n\n BaseAuto.predict (dataset, step_size=1, **data_kwargs)\n\nBaseAuto.predict\nPredictions of the best performing model on validation.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset see details here step_size: int, steps between sequential predictions, (default 1). **data_kwarg: additional parameters for the dataset module.\nReturns: y_hat: numpy predictions of the NeuralForecast model.\n\n\nReferences\n\nJames Bergstra, Remi Bardenet, Yoshua Bengio, and Balazs Kegl (2011). “Algorithms for Hyper-Parameter Optimization”. In: Advances in Neural Information Processing Systems. url: https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf\nKirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christopher R. Collins, Jeff Schneider, Barnabas Poczos, Eric P. Xing (2019). “Tuning Hyperparameters without Grad Students: Scalable and Robust Bayesian Optimisation with Dragonfly”. Journal of Machine Learning Research. url: https://arxiv.org/abs/1903.06694\nLisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, Ameet Talwalkar (2016). “Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization”. Journal of Machine Learning Research. url: https://arxiv.org/abs/1603.06560"
  },
  {
    "objectID": "common.base_windows.html",
    "href": "common.base_windows.html",
    "title": "BaseWindows",
    "section": "",
    "text": "BaseWindows\n\n BaseWindows (h, input_size, loss, learning_rate, batch_size=32,\n              windows_batch_size=1024, step_size=1,\n              scaler_type='identity', futr_exog_list=None,\n              hist_exog_list=None, stat_exog_list=None,\n              num_workers_loader=0, drop_last_loader=False, random_seed=1,\n              **trainer_kwargs)\n\nHooks to be used in LightningModule.\n\n\n\nBaseWindows.fit\n\n BaseWindows.fit (dataset, val_size=0, test_size=0)\n\nFit.\nThe fit method, optimizes the neural network’s weights using the initialization parameters (learning_rate, windows_batch_size, …) and the loss function as defined during the initialization. Within fit we use a PyTorch Lightning Trainer that inherits the initialization’s self.trainer_kwargs, to customize its inputs, see PL’s trainer arguments.\nThe method is designed to be compatible with SKLearn-like classes and in particular to be compatible with the StatsForecast library.\nBy default the model is not saving training checkpoints to protect disk memory, to get them change enable_checkpointing=True in __init__.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation. val_size: int, validation size for temporal cross-validation. test_size: int, test size for temporal cross-validation.\n\n\n\nBaseWindows.predict\n\n BaseWindows.predict (dataset, test_size=None, step_size=1,\n                      **data_module_kwargs)\n\nPredict.\nNeural network prediction with PL’s Trainer execution of predict_step.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation. test_size: int=None, test size for temporal cross-validation. step_size: int=1, Step size between each window. **data_module_kwargs: PL’s TimeSeriesDataModule args, see documentation.\n\n\n\nBaseWindows.decompose\n\n BaseWindows.decompose (dataset, step_size=1, **data_module_kwargs)\n\nDecompose Predictions.\nDecompose the predictions through the network’s layers. Available methods are ESRNN, NHITS, NBEATS, and NBEATSx.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation here. step_size: int=1, step size between each window of temporal data. **data_module_kwargs: PL’s TimeSeriesDataModule args, see documentation."
  },
  {
    "objectID": "models.dilated_rnn.html",
    "href": "models.dilated_rnn.html",
    "title": "Dilated RNN",
    "section": "",
    "text": "The Dilated Recurrent Neural Network (DilatedRNN) addresses common challenges of modeling long sequences like vanishing gradients, computational efficiency, and improved model flexibility to model complex relationships while maintaining its parsimony. The DilatedRNN builds a deep stack of RNN layers using skip conditions on the temporal and the network’s depth dimensions. The temporal dilated recurrent skip connections offer the capability to focus on multi-resolution inputs.The predictions are obtained by transforming the hidden states into contexts \\(\\mathbf{c}_{[t+1:t+H]}\\), that are decoded and adapted into \\(\\mathbf{\\hat{y}}_{[t+1:t+H],[q]}\\) through MLPs.\n\\[\\begin{align}\n\\mathbf{h}_{t} &= \\textrm{DilatedRNN}([\\mathbf{y}_{t},\\mathbf{x}^{(h)}_{t},\\mathbf{x}^{(s)}], \\mathbf{h}_{t-1})\\\\\n\\mathbf{c}_{[t+1:t+H]}&=\\textrm{Linear}([\\mathbf{h}_{t}, \\mathbf{x}^{(f)}_{[:t+H]}]) \\\\\n\\hat{y}_{\\tau,[q]}&=\\textrm{MLP}([\\mathbf{c}_{\\tau},\\mathbf{x}^{(f)}_{\\tau}])\n\\end{align}\\]\nwhere \\(\\mathbf{h}_{t}\\), is the hidden state for time \\(t\\), \\(\\mathbf{y}_{t}\\) is the input at time \\(t\\) and \\(\\mathbf{h}_{t-1}\\) is the hidden state of the previous layer at \\(t-1\\), \\(\\mathbf{x}^{(s)}\\) are static exogenous inputs, \\(\\mathbf{x}^{(h)}_{t}\\) historic exogenous, \\(\\mathbf{x}^{(f)}_{[:t+H]}\\) are future exogenous available at the time of the prediction.\nReferences-Shiyu Chang, et al. “Dilated Recurrent Neural Networks”.-Yao Qin, et al. “A Dual-Stage Attention-Based recurrent neural network for time series prediction”.-Kashif Rasul, et al. “Zalando Research: PyTorch Dilated Recurrent Neural Networks”.\nsource"
  },
  {
    "objectID": "models.dilated_rnn.html#usage-example",
    "href": "models.dilated_rnn.html#usage-example",
    "title": "Dilated RNN",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nfrom neuralforecast.tsdataset import TimeSeriesDataset, TimeSeriesLoader\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nfcst = NeuralForecast(\n    models=[DilatedRNN(h=12, input_size=-1,\n                       #loss=MAE(),\n                       #loss=MQLoss(level=[80, 90]),\n                       loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n                       scaler_type='robust',\n                       encoder_hidden_size=100,\n                       max_epochs=200,\n                       futr_exog_list=['y_[lag12]'],\n                       hist_exog_list=None,\n                       stat_exog_list=['airline1'],\n    )\n    ],\n    freq='M'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\ntry:\n    plt.plot(plot_df['ds'], plot_df['DilatedRNN-median'], c='blue', label='median')\n    plt.fill_between(x=plot_df['ds'], \n                     y1=plot_df['DilatedRNN-lo-90.0'], y2=plot_df['DilatedRNN-hi-90.0'],\n                     alpha=0.4, label='level 90')\nexcept:\n    plt.plot(plot_df['ds'], plot_df['DilatedRNN'], c='blue', label='median')\nplt.legend()\nplt.grid()\nplt.plot()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "🧠 Neural Forecast",
    "section": "",
    "text": "Exogenous Variables: Static, historic and future exogenous support.\nForecast Interpretability: Plot trend, seasonality and exogenous NBEATS, NHITS, TFT, ESRNN prediction components.\nProbabilistic Forecasting: Simple model adapters for quantile losses and parametric distributions.\nTrain and Evaluation Losses Scale-dependent, percentage and scale independent errors, and parametric likelihoods.\nAutomatic Model Selection Parallelized automatic hyperparameter tuning, that efficiently searches best validation configuration.\nSimple Interface Unified SKLearn Interface for StatsForecast and MLForecast compatibility.\nModel Collection: Out of the box implementation of MLP, LSTM, RNN, TCN, DilatedRNN, NBEATS, NHITS, ESRNN, AutoFormer, Informer, TFT, AutoFormer, Informer, and vanilla Transformer. See the entire collection here."
  },
  {
    "objectID": "index.html#why",
    "href": "index.html#why",
    "title": "🧠 Neural Forecast",
    "section": "Why?",
    "text": "Why?\nThere is a shared belief in Neural forecasting methods’ capacity to improve our pipeline’s accuracy and efficiency.\nUnfortunately, available implementations and published research are yet to realize neural networks’ potential. They are hard to use and continuously fail to improve over statistical methods while being computationally prohibitive. For this reason, we created NeuralForecast, a library favoring proven accurate and efficient models focusing on their usability."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "🧠 Neural Forecast",
    "section": "💻 Installation",
    "text": "💻 Installation\n\nPyPI\nYou can install NeuralForecast’s released version from the Python package index pip with:\npip install neuralforecast\n(Installing inside a python virtualenvironment or a conda environment is recommended.)\n\n\nConda\nAlso you can install NeuralForecast’s released version from conda with:\nconda install -c conda-forge neuralforecast\n(Installing inside a python virtualenvironment or a conda environment is recommended.)\n\n\nDev Mode\nIf you want to make some modifications to the code and see the effects in real time (without reinstalling), follow the steps below:\ngit clone https://github.com/Nixtla/neuralforecast.git\ncd neuralforecast\npip install -e ."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "🧠 Neural Forecast",
    "section": "How to Use",
    "text": "How to Use"
  },
  {
    "objectID": "index.html#how-to-cite",
    "href": "index.html#how-to-cite",
    "title": "🧠 Neural Forecast",
    "section": "How to cite",
    "text": "How to cite"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": " Core ",
    "section": "",
    "text": "source\n\nNeuralForecast\n\n NeuralForecast (models:List[Any], freq:str)\n\nThe core.StatsForecast class allows you to efficiently fit multiple NeuralForecast models for large sets of time series. It operates with pandas DataFrame df that identifies series and datestamps with the unique_id and ds columns. The y column denotes the target time series variable.\nParameters: h: int, forecast horizon. models: List[typing.Any], instantiated neuralforecast.models see collection here. freq: str, frequency of the data, panda’s available frequencies. trainers: List[typing.Any], optional list of instantiated pytorch lightning trainers.\n\nsource\n\n\nNeuralForecast.fit\n\n NeuralForecast.fit (df:Optional[pandas.core.frame.DataFrame]=None,\n                     static_df:Optional[pandas.core.frame.DataFrame]=None,\n                     val_size:Optional[int]=0, sort_df:bool=True,\n                     verbose:bool=False)\n\nFit the core.NeuralForecast.\nFit models to a large set of time series from DataFrame df. and store fitted models for later inspection.\nParameters: df: pandas.DataFrame, with columns [unique_id, ds, y] and exogenous. static_df: pandas.DataFrame, with columns [unique_id, ds] and static exogenous. val_size: int, size of validation set. sort_df: bool, sort df before fitting.\nReturns: self: Returns with stored NeuralForecast fitted models.\n\nsource\n\n\nNeuralForecast.predict\n\n NeuralForecast.predict (df:Optional[pandas.core.frame.DataFrame]=None,\n                         static_df:Optional[pandas.core.frame.DataFrame]=N\n                         one, futr_df:Optional[pandas.core.frame.DataFrame\n                         ]=None, sort_df:bool=True, verbose:bool=False,\n                         **data_kwargs)\n\nPredict with core.NeuralForecast.\nUse stored fitted models to predict large set of time series from DataFrame df.\nParameters: df: pandas.DataFrame, with columns [unique_id, ds, y] and exogenous. static_df: pandas.DataFrame, with columns [unique_id, ds] and static exogenous. futr_df: pandas.DataFrame, with [unique_id, ds] columns and df’s future exogenous.\nReturns: fcsts_df: pandas.DataFrame, with models columns for point predictions.\n\nsource\n\n\nNeuralForecast.cross_validation\n\n NeuralForecast.cross_validation (df:pandas.core.frame.DataFrame=None,\n                                  static_df:Optional[pandas.core.frame.Dat\n                                  aFrame]=None, n_windows:int=1,\n                                  step_size:int=1,\n                                  val_size:Optional[int]=0,\n                                  test_size:Optional[int]=None,\n                                  sort_df:bool=True, verbose:bool=False,\n                                  **data_kwargs)\n\nTemporal Cross-Validation with core.NeuralForecast.\ncore.NeuralForecast’s cross-validation efficiently fits a list of NeuralForecast models through multiple windows, in either chained or rolled manner.\nParameters: df: pandas.DataFrame, with columns [unique_id, ds, y] and exogenous. static_df: pandas.DataFrame, with columns [unique_id, ds] and static exogenous. n_windows: int, number of windows used for cross validation. step_size: int = 1, step size between each window. val_size: Optional[int] = None, length of validation size. If passed, set n_windows=None. test_size: Optional[int] = None, length of test size. If passed, set n_windows=None.\nReturns: fcsts_df: pandas.DataFrame, with insample models columns for point predictions and probabilistic predictions for all fitted models."
  },
  {
    "objectID": "models.rnn.html",
    "href": "models.rnn.html",
    "title": "RNN",
    "section": "",
    "text": "Elman proposed this classic recurrent neural network (RNN) in 1990, where each layer uses the following recurrent transformation: \\[\\mathbf{h}^{l}_{t} = \\mathrm{Activation}([\\mathbf{y}_{t},\\mathbf{x}^{(h)}_{t},\\mathbf{x}^{(s)}] W^{\\intercal}_{ih} + b_{ih}  +  \\mathbf{h}^{l}_{t-1} W^{\\intercal}_{hh} + b_{hh})\\]\nwhere \\(\\mathbf{h}^{l}_{t}\\), is the hidden state of RNN layer \\(l\\) for time \\(t\\), \\(\\mathbf{y}_{t}\\) is the input at time \\(t\\) and \\(\\mathbf{h}_{t-1}\\) is the hidden state of the previous layer at \\(t-1\\), \\(\\mathbf{x}^{(s)}\\) are static exogenous inputs, \\(\\mathbf{x}^{(h)}_{t}\\) historic exogenous, \\(\\mathbf{x}^{(f)}_{[:t+H]}\\) are future exogenous available at the time of the prediction. The available activations are tanh, and relu. The predictions are obtained by transforming the hidden states into contexts \\(\\mathbf{c}_{[t+1:t+H]}\\), that are decoded and adapted into \\(\\mathbf{\\hat{y}}_{[t+1:t+H],[q]}\\) through MLPs.\n\\[\\begin{align}\n\\mathbf{h}_{t} &= \\textrm{RNN}([\\mathbf{y}_{t},\\mathbf{x}^{(h)}_{t},\\mathbf{x}^{(s)}], \\mathbf{h}_{t-1})\\\\\n\\mathbf{c}_{[t+1:t+H]}&=\\textrm{Linear}([\\mathbf{h}_{t}, \\mathbf{x}^{(f)}_{[:t+H]}]) \\\\\n\\hat{y}_{\\tau,[q]}&=\\textrm{MLP}([\\mathbf{c}_{\\tau},\\mathbf{x}^{(f)}_{\\tau}])\n\\end{align}\\]\nReferences -Jeffrey L. Elman (1990). “Finding Structure in Time”. -Cho, K., van Merrienboer, B., Gülcehre, C., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation.\nsource"
  },
  {
    "objectID": "models.rnn.html#usage-example",
    "href": "models.rnn.html#usage-example",
    "title": "RNN",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nfrom neuralforecast.tsdataset import TimeSeriesDataset, TimeSeriesLoader\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nfcst = NeuralForecast(\n    models=[RNN(h=12, input_size=-1,\n                #loss=MAE(),\n                loss=MQLoss(level=[80, 90]),\n                #loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n                scaler_type='robust',\n                encoder_n_layers=2,\n                encoder_hidden_size=128,\n                context_size=10,\n                decoder_hidden_size=128,\n                decoder_layers=2,\n                max_epochs=500,\n                futr_exog_list=None,\n                hist_exog_list=['y_[lag12]'],\n                stat_exog_list=['airline1'],\n                )\n    ],\n    freq='M'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\ntry:\n    plt.plot(plot_df['ds'], plot_df['RNN-median'], c='blue', label='median')\n    plt.fill_between(x=plot_df['ds'], \n                     y1=plot_df['RNN-lo-90.0'], y2=plot_df['RNN-hi-90.0'],\n                     alpha=0.4, label='level 90')\nexcept:\n    plt.plot(plot_df['ds'], plot_df['RNN'], c='blue', label='median')\nplt.legend()\nplt.grid()\nplt.plot()"
  },
  {
    "objectID": "models.nhits.html",
    "href": "models.nhits.html",
    "title": "NHITS",
    "section": "",
    "text": "Long-horizon forecasting is challenging because of the volatility of the predictions and the computational complexity. To solve this problem we created the Neural Hierarchical Interpolation for Time Series (NHITS). NHITS builds upon NBEATS and specializes its partial outputs in the different frequencies of the time series through hierarchical interpolation and multi-rate input processing. On the long-horizon forecasting task NHITS improved accuracy by 25% on AAAI’s best paper award the Informer, while being 50x faster.\nThe model is composed of several MLPs with ReLU non-linearities. Blocks are connected via doubly residual stacking principle with the backcast \\(\\mathbf{\\tilde{y}}_{t-L:t,l}\\) and forecast \\(\\mathbf{\\hat{y}}_{t+1:t+H,l}\\) outputs of the l-th block. Multi-rate input pooling, hierarchical interpolation and backcast residual connections together induce the specialization of the additive predictions in different signal bands, reducing memory footprint and computational time, thus improving the architecture parsimony and accuracy.\nReferences -Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio (2019). “N-BEATS: Neural basis expansion analysis for interpretable time series forecasting”. -Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2022). “N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting”. Accepted at the Thirty-Seventh AAAI Conference on Artificial Intelligence. -Zhou, H.; Zhang, S.; Peng, J.; Zhang, S.; Li, J.; Xiong, H.; and Zhang, W. (2020). “Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting”. Association for the Advancement of Artificial Intelligence Conference 2021 (AAAI 2021).\nsource"
  },
  {
    "objectID": "models.nhits.html#usage-example",
    "href": "models.nhits.html#usage-example",
    "title": "NHITS",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.losses.pytorch import MQLoss, PMM, DistributionLoss\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic\n\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = NHITS(h=12, input_size=24,\n              #loss=MAE(),\n              #loss=MQLoss(level=[80, 90]),\n              #loss=DistributionLoss(distribution='Poisson', level=[80, 90]),\n              loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n              stat_exog_list=['airline1'],\n              futr_exog_list=['trend'],\n              n_freq_downsample=[2, 1, 1],\n              scaler_type='robust',\n              #scaler_type='identity',\n              learning_rate=1e-3,\n              max_epochs=100)\n\nfcst = NeuralForecast(\n    models=[model],\n    freq='M'\n)\n\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot quantile predictions\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\ntry:\n    plt.plot(plot_df['ds'], plot_df['NHITS-median'], c='blue', label='median')\n    plt.fill_between(x=plot_df['ds'], \n                     y1=plot_df['NHITS-lo-90.0'], y2=plot_df['NHITS-hi-90.0'],\n                     alpha=0.4, label='level 90')\nexcept:\n    plt.plot(plot_df['ds'], plot_df['NHITS'], c='blue', label='median')\nplt.legend()\nplt.grid()\nplt.plot()"
  },
  {
    "objectID": "common.base_recurrent.html",
    "href": "common.base_recurrent.html",
    "title": "BaseRecurrent",
    "section": "",
    "text": "The BaseRecurrent class contains standard methods shared across recurrent neural networks; these models possess the ability to process variable-length sequences of inputs through their internal memory states. The class is represented by LSTM, GRU, and RNN, along with other more sophisticated architectures like MQCNN.\nThe standard methods include TemporalNorm preprocessing, optimization utilities like parameter initialization, training_step, validation_step, and shared fit and predict methods.These shared methods enable all the neuralforecast.models compatibility with the core.NeuralForecast wrapper class.\n\n\nBaseRecurrent\n\n BaseRecurrent (h, input_size, loss, learning_rate, batch_size=32,\n                scaler_type='robust', futr_exog_list=None,\n                hist_exog_list=None, stat_exog_list=None,\n                num_workers_loader=0, drop_last_loader=False,\n                random_seed=1, **trainer_kwargs)\n\nHooks to be used in LightningModule.\n\n\n\nBaseRecurrent.fit\n\n BaseRecurrent.fit (dataset, val_size=0, test_size=0)\n\nFit.\nThe fit method, optimizes the neural network’s weights using the initialization parameters (learning_rate, batch_size, …) and the loss function as defined during the initialization. Within fit we use a PyTorch Lightning Trainer that inherits the initialization’s self.trainer_kwargs, to customize its inputs, see PL’s trainer arguments.\nThe method is designed to be compatible with SKLearn-like classes and in particular to be compatible with the StatsForecast library.\nBy default the model is not saving training checkpoints to protect disk memory, to get them change enable_checkpointing=True in __init__.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation. val_size: int, validation size for temporal cross-validation. test_size: int, test size for temporal cross-validation.\n\n\n\nBaseRecurrent.predict\n\n BaseRecurrent.predict (dataset, step_size=1, **data_module_kwargs)\n\nPredict.\nNeural network prediction with PL’s Trainer execution of predict_step.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation. step_size: int=1, Step size between each window. **data_module_kwargs: PL’s TimeSeriesDataModule args, see documentation."
  },
  {
    "objectID": "models.gru.html",
    "href": "models.gru.html",
    "title": "GRU",
    "section": "",
    "text": "Cho et. al proposed the Gated Recurrent Unit (GRU) to improve on LSTM and Elman cells. The predictions at each time are given by a MLP decoder. This architecture follows closely the original Multi Layer Elman RNN with the main difference being its use of the GRU cells. The predictions are obtained by transforming the hidden states into contexts \\(\\mathbf{c}_{[t+1:t+H]}\\), that are decoded and adapted into \\(\\mathbf{\\hat{y}}_{[t+1:t+H],[q]}\\) through MLPs.\n\\[\\begin{align}\n\\mathbf{h}_{t} &= \\textrm{GRU}([\\mathbf{y}_{t},\\mathbf{x}^{(h)}_{t},\\mathbf{x}^{(s)}], \\mathbf{h}_{t-1})\\\\\n\\mathbf{c}_{[t+1:t+H]}&=\\textrm{Linear}([\\mathbf{h}_{t}, \\mathbf{x}^{(f)}_{[:t+H]}]) \\\\\n\\hat{y}_{\\tau,[q]}&=\\textrm{MLP}([\\mathbf{c}_{\\tau},\\mathbf{x}^{(f)}_{\\tau}])\n\\end{align}\\]\nwhere \\(\\mathbf{h}_{t}\\), is the hidden state for time \\(t\\), \\(\\mathbf{y}_{t}\\) is the input at time \\(t\\) and \\(\\mathbf{h}_{t-1}\\) is the hidden state of the previous layer at \\(t-1\\), \\(\\mathbf{x}^{(s)}\\) are static exogenous inputs, \\(\\mathbf{x}^{(h)}_{t}\\) historic exogenous, \\(\\mathbf{x}^{(f)}_{[:t+H]}\\) are future exogenous available at the time of the prediction.\nReferences -Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio (2014). “Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling”. -Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, Yoshua Bengio (2014). “On the Properties of Neural Machine Translation: Encoder-Decoder Approaches”.\nsource"
  },
  {
    "objectID": "models.gru.html#usage-example",
    "href": "models.gru.html#usage-example",
    "title": "GRU",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nfrom neuralforecast.tsdataset import TimeSeriesDataset, TimeSeriesLoader\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nfcst = NeuralForecast(\n    models=[GRU(h=12,input_size=-1,\n                #loss=MAE(),\n                #loss=MQLoss(level=[80, 90]),\n                loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n                scaler_type='robust',\n                encoder_n_layers=2,\n                encoder_hidden_size=128,\n                context_size=10,\n                decoder_hidden_size=128,\n                decoder_layers=2,\n                max_epochs=100,\n                futr_exog_list=None,\n                hist_exog_list=['y_[lag12]'],\n                stat_exog_list=['airline1'],\n                )\n    ],\n    freq='M'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\ntry:\n    plt.plot(plot_df['ds'], plot_df['GRU-median'], c='blue', label='median')\n    plt.fill_between(x=plot_df['ds'], \n                     y1=plot_df['GRU-lo-90.0'], y2=plot_df['GRU-hi-90.0'],\n                     alpha=0.4, label='level 90')\nexcept:\n    plt.plot(plot_df['ds'], plot_df['GRU'], c='blue', label='median')\nplt.legend()\nplt.grid()\nplt.plot()"
  },
  {
    "objectID": "models.nbeats.html",
    "href": "models.nbeats.html",
    "title": "NBEATS",
    "section": "",
    "text": "The Neural Basis Expansion Analysis (NBEATS) is an MLP-based deep neural architecture with backward and forward residual links. The network has two variants: (1) in its interpretable configuration, NBEATS sequentially projects the signal into polynomials and harmonic basis to learn trend and seasonality components; (2) in its generic configuration, it substitutes the polynomial and harmonic basis for identity basis and larger network’s depth. The Neural Basis Expansion Analysis with Exogenous (NBEATSx), incorporates projections to exogenous temporal variables available at the time of the prediction.\nThis method proved state-of-the-art performance on the M3, M4, and Tourism Competition datasets, improving accuracy by 3% over the ESRNN M4 competition winner.\nReferences -Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio (2019). “N-BEATS: Neural basis expansion analysis for interpretable time series forecasting”.\nsource"
  },
  {
    "objectID": "models.nbeats.html#usage-example",
    "href": "models.nbeats.html#usage-example",
    "title": "NBEATS",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = NBEATS(h=12, input_size=24,\n               #loss=MQLoss(level=[80, 90]),\n               loss=DistributionLoss(distribution='Poisson', level=[80, 90]),\n               stack_types = ['identity', 'trend', 'seasonality'],\n               max_epochs=100)\n\nfcst = NeuralForecast(\n    models=[model],\n    freq='M'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot quantile predictions\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['NBEATS-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'], \n                 y1=plot_df['NBEATS-lo-90.0'], y2=plot_df['NBEATS-hi-90.0'],\n                 alpha=0.4, label='level 90.0')\nplt.grid()\nplt.legend()\nplt.plot()"
  },
  {
    "objectID": "models.tft.html",
    "href": "models.tft.html",
    "title": "TFT",
    "section": "",
    "text": "In summary Temporal Fusion Transformer (TFT) combines gating layers, an LSTM recurrent encoder, with multi-head attention layers for a multi-step forecasting strategy decoder.TFT’s inputs are static exogenous \\(\\mathbf{x}^{(s)}\\), historic exogenous \\(\\mathbf{x}^{(h)}_{[:t]}\\), exogenous available at the time of the prediction \\(\\mathbf{x}^{(f)}_{[:t+H]}\\) and autorregresive features \\(\\mathbf{y}_{[:t]}\\), each of these inputs is further decomposed into categorical and continuous. The network uses a multi-quantile regression to model the following conditional probability:\\[\\mathbb{P}(\\mathbf{y}_{[t+1:t+H]}|\\;\\mathbf{y}_{[:t]},\\; \\mathbf{x}^{(h)}_{[:t]},\\; \\mathbf{x}^{(f)}_{[:t+H]},\\; \\mathbf{x}^{(s)})\\]\nReferences - Jan Golda, Krzysztof Kudrynski. “NVIDIA, Deep Learning Forecasting Examples” - Bryan Lim, Sercan O. Arik, Nicolas Loeff, Tomas Pfister, “Temporal Fusion Transformers for interpretable multi-horizon time series forecasting”"
  },
  {
    "objectID": "models.tft.html#auxiliary-functions",
    "href": "models.tft.html#auxiliary-functions",
    "title": "TFT",
    "section": "1. Auxiliary Functions",
    "text": "1. Auxiliary Functions\n\n1.1 Gating Mechanisms\nThe Gated Residual Network (GRN) provides adaptive depth and network complexity capable of accommodating different size datasets. As residual connections allow for the network to skip the non-linear transformation of input \\(\\mathbf{a}\\) and context \\(\\mathbf{c}\\).\n\\[\\begin{align}\n\\eta_{1} &= \\mathrm{ELU}(\\mathbf{W}_{1}\\mathbf{a}+\\mathbf{W}_{2}\\mathbf{c}+\\mathbf{b}_{1}) \\\\\n\\eta_{2} &= \\mathbf{W}_{2}\\eta_{1}+b_{2} \\\\\n\\mathrm{GRN}(\\mathbf{a}, \\mathbf{c}) &= \\mathrm{LayerNorm}(a + \\textrm{GLU}(\\eta_{2}))\n\\end{align}\\]\nThe Gated Linear Unit (GLU) provides the flexibility of supressing unnecesary parts of the GRN. Consider GRN’s output \\(\\gamma\\) then GLU transformation is defined by:\n\\[\\mathrm{GLU}(\\gamma) = \\sigma(\\mathbf{W}_{4}\\gamma +b_{4}) \\odot (\\mathbf{W}_{5}\\gamma +b_{5})\\]\n\n\n\nFigure 2. Gated Residual Network.\n\n\n\n\n1.2 Variable Selection Networks\nTFT includes automated variable selection capabilities, through its variable selection network (VSN) components. The VSN takes the original input \\(\\{\\mathbf{x}^{(s)}, \\mathbf{x}^{(h)}_{[:t]}, \\mathbf{x}^{(f)}_{[:t]}\\}\\) and transforms it through embeddings or linear transformations into a high dimensional space \\(\\{\\mathbf{E}^{(s)}, \\mathbf{E}^{(h)}_{[:t]}, \\mathbf{E}^{(f)}_{[:t+H]}\\}\\).\nFor the observed historic data, the embedding matrix \\(\\mathbf{E}^{(h)}_{t}\\) at time \\(t\\) is a concatenation of \\(j\\) variable \\(e^{(h)}_{t,j}\\) embeddings: \\[\\begin{align}\n\\mathbf{E}^{(h)}_{t} &= [e^{(h)}_{t,1},\\dots,e^{(h)}_{t,j},\\dots,e^{(h)}_{t,n_{h}}] \\\\\n\\mathbf{\\tilde{e}}^{(h)}_{t,j} &= \\mathrm{GRN}(e^{(h)}_{t,j})\n\\end{align}\\]\nThe variable selection weights are given by: \\[s^{(h)}_{t}=\\mathrm{SoftMax}(\\mathrm{GRN}(\\mathbf{E}^{(h)}_{t},\\mathbf{E}^{(s)}))\\]\nThe VSN processed features are then: \\[\\tilde{\\mathbf{E}}^{(h)}_{t}= \\sum_{j} s^{(h)}_{j} \\tilde{e}^{(h)}_{t,j}\\]\n\n\n\nFigure 3. Variable Selection Network.\n\n\n\n\n1.3. Multi-Head Attention\nTo avoid information bottlenecks from the classic Seq2Seq architecture, TFT incorporates a decoder-encoder attention mechanism inherited transformer architectures (Li et. al 2019, Vaswani et. al 2017). It transform the the outputs of the LSTM encoded temporal features, and helps the decoder better capture long-term relationships.\nThe original multihead attention for each component \\(H_{m}\\) and its query, key, and value representations are denoted by \\(Q_{m}, K_{m}, V_{m}\\), its transformation is given by:\n\\[\\begin{align}\nQ_{m} = Q W_{Q,m} \\quad K_{m} = K W_{K,h} \\quad V_{m} = V W_{V,m} \\\\\nH_{m}=\\mathrm{Attention}(Q_{m}, K_{m}, V_{m}) = \\mathrm{SoftMax}(Q_{m} K^{\\intercal}_{m}/\\mathrm{scale}) \\; V_{m} \\\\\n\\mathrm{MultiHead}(Q, K, V) = [H_{1},\\dots,H_{M}] W_{M}\n\\end{align}\\]\nTFT modifies the original multihead attention to improve its interpretability. To do it it uses shared values \\(\\tilde{V}\\) across heads and employs additive aggregation, \\(\\mathrm{InterpretableMultiHead}(Q,K,V) = \\tilde{H} W_{M}\\). The mechanism has a great resemblence to a single attention layer, but it allows for \\(M\\) multiple attention weights, and can be therefore be interpreted as the average ensemble of \\(M\\) single attention layers.\n\\[\\begin{align}\n\\tilde{H} &= \\left(\\frac{1}{M} \\sum_{m} \\mathrm{SoftMax}(Q_{m} K^{\\intercal}_{m}/\\mathrm{scale}) \\right) \\tilde{V}\n          = \\frac{1}{M} \\sum_{m} \\mathrm{Attention}(Q_{m}, K_{m}, \\tilde{V}) \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "models.tft.html#tft-architecture",
    "href": "models.tft.html#tft-architecture",
    "title": "TFT",
    "section": "2. TFT Architecture",
    "text": "2. TFT Architecture\nThe first TFT’s step is embed the original input \\(\\{\\mathbf{x}^{(s)}, \\mathbf{x}^{(h)}, \\mathbf{x}^{(f)}\\}\\) into a high dimensional space \\(\\{\\mathbf{E}^{(s)}, \\mathbf{E}^{(h)}, \\mathbf{E}^{(f)}\\}\\), after which each embedding is gated by a variable selection network (VSN). The static embedding \\(\\mathbf{E}^{(s)}\\) is used as context for variable selection and as initial condition to the LSTM. Finally the encoded variables are fed into the multi-head attention decoder.\n\\[\\begin{align}\nc_{s}, c_{e}, (c_{h}, c_{c}) &=\\textrm{StaticCovariateEncoder}(\\mathbf{E}^{(s)}) \\\\\n      h_{[:t]}, h_{[t+1:t+H]}  &=\\textrm{TemporalCovariateEncoder}(\\mathbf{E}^{(h)}, \\mathbf{E}^{(f)}, c_{h}, c_{c}) \\\\\n\\hat{\\mathbf{y}}^{(q)}_{[t+1:t+H]} &=\\textrm{TemporalFusionDecoder}(h_{[t+1:t+H]}, c_{e})\n\\end{align}\\]\n\n2.1 Static Covariate Encoder\nThe static embedding \\(\\mathbf{E}^{(s)}\\) is transformed by the StaticCovariateEncoder into contexts \\(c_{s}, c_{e}, c_{h}, c_{c}\\). Where \\(c_{s}\\) are temporal variable selection contexts, \\(c_{e}\\) are TemporalFusionDecoder enriching contexts, and \\(c_{h}, c_{c}\\) are LSTM’s hidden/contexts for the TemporalCovariateEncoder.\n\\[\\begin{align}\nc_{s}, c_{e}, (c_{h}, c_{c}) & = \\textrm{GRN}(\\textrm{VSN}(\\mathbf{E}^{(s)}))\n\\end{align}\\]\n\n\n2.2 Temporal Covariate Encoder\nTemporalCovariateEncoder encodes the embeddings \\(\\mathbf{E}^{(h)}, \\mathbf{E}^{(f)}\\) and contexts \\((c_{h}, c_{c})\\) with an LSTM.\n\\[\\begin{align}\n\\tilde{\\mathbf{E}}^{(h)}_{[:t]} & = \\textrm{VSN}(\\mathbf{E}^{(h)}_{[:t]}, c_{s}) \\\\\n\\tilde{\\mathbf{E}}^{(h)}_{[:t]} &= \\mathrm{LSTM}(\\tilde{\\mathbf{E}}^{(h)}_{[:t]}, (c_{h}, c_{c})) \\\\\nh_{[:t]} &= \\mathrm{Gate}(\\mathrm{LayerNorm}(\\tilde{\\mathbf{E}}^{(h)}_{[:t]}))\n\\end{align}\\]\nAn analogous process is repeated for the future data, with the main difference that \\(\\mathbf{E}^{(f)}\\) contains the future available information.\n\\[\\begin{align}\n\\tilde{\\mathbf{E}}^{(f)}_{[t+1:t+h]} & = \\textrm{VSN}(\\mathbf{E}^{(h)}_{t+1:t+H}, \\mathbf{E}^{(f)}_{t+1:t+H}, c_{s}) \\\\\n\\tilde{\\mathbf{E}}^{(f)}_{[t+1:t+h]} &= \\mathrm{LSTM}(\\tilde{\\mathbf{E}}^{(h)}_{[t+1:t+h]}, (c_{h}, c_{c})) \\\\\nh_{[t+1:t+H]} &= \\mathrm{Gate}(\\mathrm{LayerNorm}(\\tilde{\\mathbf{E}}^{(f)}_{[t+1:t+h]}))\n\\end{align}\\]\n\n\n2.3 Temporal Fusion Decoder\nThe TemporalFusionDecoder enriches the LSTM’s outputs with \\(c_{e}\\) and then uses an attention layer, and multi-step adapter. \\[\\begin{align}\nh_{[t+1:t+H]} &= \\mathrm{MultiHeadAttention}(h_{[:t]}, h_{[t+1:t+H]}, c_{e}) \\\\\nh_{[t+1:t+H]} &= \\mathrm{Gate}(\\mathrm{LayerNorm}(h_{[t+1:t+H]}) \\\\\nh_{[t+1:t+H]} &= \\mathrm{Gate}(\\mathrm{LayerNorm}(\\mathrm{GRN}(h_{[t+1:t+H]})) \\\\\n\\hat{\\mathbf{y}}^{(q)}_{[t+1:t+H]} &= \\mathrm{MLP}(h_{[t+1:t+H]})\n\\end{align}\\]"
  },
  {
    "objectID": "models.tft.html#tft-methods",
    "href": "models.tft.html#tft-methods",
    "title": "TFT",
    "section": "3. TFT methods",
    "text": "3. TFT methods\n\nsource\n\nTFT\n\n TFT (h, input_size, tgt_size=1, stat_exog_list=None, hist_exog_list=None,\n      futr_exog_list=None, hidden_size=128, n_head=4, attn_dropout=0.0,\n      dropout=0.1, loss=MAE(), learning_rate=0.001, batch_size=32,\n      windows_batch_size=1024, step_size=1, scaler_type='robust',\n      num_workers_loader=0, drop_last_loader=False, random_seed=1,\n      **trainer_kwargs)\n\nTFT\nThe Temporal Fusion Transformer architecture (TFT) is an Sequence-to-Sequence model that combines static, historic and future available data to predict an univariate target. The method combines gating layers, an LSTM recurrent encoder, with and interpretable multi-head attention layer and a multi-step forecasting strategy decoder.\nParameters: h: int, Forecast horizon.  input_size: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2]. stat_exog_list: str list, static continuous columns. hist_exog_list: str list, historic continuous columns. futr_exog_list: str list, future continuous columns. hidden_size: int, units of embeddings and encoders. dropout: float (0, 1), dropout of inputs VSNs. attn_dropout: float (0, 1), dropout of fusion decoder’s attention layer. shared_weights: bool, If True, all blocks within each stack will share parameters.  activation: str, activation from [‘ReLU’, ‘Softplus’, ‘Tanh’, ‘SELU’, ‘LeakyReLU’, ‘PReLU’, ‘Sigmoid’]. loss: PyTorch module, instantiated train loss class from losses collection. learning_rate: float (0, 1), initial optimization learning rate. batch_size: int, number of different series in each batch. windows_batch_size: int=None, windows sampled from rolled data, default uses all. step_size: int=1, step size between each window of temporal data. scaler_type: str=‘robust’, type of scaler for temporal inputs normalization see temporal scalers. random_seed: int, random seed initialization for replicability. num_workers_loader: int=os.cpu_count(), workers to be used by TimeSeriesDataLoader. drop_last_loader: bool=False, if True TimeSeriesDataLoader drops last non-full batch. **trainer_kwargs: int, keyword trainer arguments inherited from PyTorch Lighning’s trainer.\nReferences: - Bryan Lim, Sercan O. Arik, Nicolas Loeff, Tomas Pfister, “Temporal Fusion Transformers for interpretable multi-horizon time series forecasting”\n\n\n\nTFT.fit\n\n TFT.fit (dataset, val_size=0, test_size=0)\n\nFit.\nThe fit method, optimizes the neural network’s weights using the initialization parameters (learning_rate, windows_batch_size, …) and the loss function as defined during the initialization. Within fit we use a PyTorch Lightning Trainer that inherits the initialization’s self.trainer_kwargs, to customize its inputs, see PL’s trainer arguments.\nThe method is designed to be compatible with SKLearn-like classes and in particular to be compatible with the StatsForecast library.\nBy default the model is not saving training checkpoints to protect disk memory, to get them change enable_checkpointing=True in __init__.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation. val_size: int, validation size for temporal cross-validation. test_size: int, test size for temporal cross-validation.\n\n\n\nTFT.predict\n\n TFT.predict (dataset, test_size=None, step_size=1, **data_module_kwargs)\n\nPredict.\nNeural network prediction with PL’s Trainer execution of predict_step.\nParameters: dataset: NeuralForecast’s TimeSeriesDataset, see documentation. test_size: int=None, test size for temporal cross-validation. step_size: int=1, Step size between each window. **data_module_kwargs: PL’s TimeSeriesDataModule args, see documentation."
  },
  {
    "objectID": "models.tft.html#usage-example",
    "href": "models.tft.html#usage-example",
    "title": "TFT",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic\n\n#AirPassengersPanel['y'] = AirPassengersPanel['y'] + 10\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nfcst = NeuralForecast(\n    models=[TFT(h=12, input_size=48,\n                hidden_size=20,\n                #loss=MAE(),\n                #loss=MQLoss(level=[80, 90]),\n                loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n                learning_rate=0.01,\n                stat_exog_list=['airline1'],\n                hist_exog_list=['y_[lag12]'],\n                futr_exog_list=['trend'],\n                max_epochs=200,\n                scaler_type='robust',\n                #scaler_type=None,\n                windows_batch_size=None,\n                enable_progress_bar=True),\n    ],\n    freq='M'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot quantile predictions\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\ntry:\n    plt.plot(plot_df['ds'], plot_df['TFT-median'], c='blue', label='median')\n    plt.fill_between(x=plot_df['ds'], \n                     y1=plot_df['TFT-lo-90.0'], y2=plot_df['TFT-hi-90.0'],\n                     alpha=0.4, label='level 90')\nexcept:\n    plt.plot(plot_df['ds'], plot_df['TFT'], c='blue', label='median')\nplt.legend()\nplt.grid()\nplt.plot()"
  },
  {
    "objectID": "models.mlp.html",
    "href": "models.mlp.html",
    "title": "MLP",
    "section": "",
    "text": "Figure 1. Three layer MLP with autorregresive inputs.\nsource"
  },
  {
    "objectID": "models.mlp.html#usage-example",
    "href": "models.mlp.html#usage-example",
    "title": "MLP",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = MLP(h=12, input_size=24,\n            #loss=MQLoss(level=[80, 90]),\n            loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n            scaler_type='robust',\n            learning_rate=1e-3,\n            max_epochs=200)\n\nfcst = NeuralForecast(\n    models=[model],\n    freq='M'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n\n# Plot quantile predictions\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['MLP-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'], \n                 y1=plot_df['MLP-lo-90.0'], y2=plot_df['MLP-hi-90.0'],\n                 alpha=0.4, label='level 90.0')\nplt.grid()\nplt.legend()\nplt.plot()"
  },
  {
    "objectID": "losses.numpy.html",
    "href": "losses.numpy.html",
    "title": "NumPy Evaluation",
    "section": "",
    "text": "These metrics are on the same scale as the data.\n\n\n\nsource\n\n\n\n mae (y:numpy.ndarray, y_hat:numpy.ndarray,\n      weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nMean Absolute Error\nCalculates Mean Absolute Error between y and y_hat. MAE measures the relative prediction accuracy of a forecasting method by calculating the deviation of the prediction and the true value at a given time and averages these devations over the length of the series.\n\\[ \\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} |y_{\\tau} - \\hat{y}_{\\tau}| \\]\nParameters: y: numpy array, Actual values. y_hat: numpy array, Predicted values. mask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: mae: numpy array, (single value).\n\n\n\n\n\n\nsource\n\n\n\n mse (y:numpy.ndarray, y_hat:numpy.ndarray,\n      weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nMean Squared Error\nCalculates Mean Squared Error between y and y_hat. MSE measures the relative prediction accuracy of a forecasting method by calculating the squared deviation of the prediction and the true value at a given time, and averages these devations over the length of the series.\n\\[ \\mathrm{MSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2} \\]\nParameters: y: numpy array, Actual values. y_hat: numpy array, Predicted values. mask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: mse: numpy array, (single value).\n\n\n\n\n\n\nsource\n\n\n\n rmse (y:numpy.ndarray, y_hat:numpy.ndarray,\n       weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nRoot Mean Squared Error\nCalculates Root Mean Squared Error between y and y_hat. RMSE measures the relative prediction accuracy of a forecasting method by calculating the squared deviation of the prediction and the observed value at a given time and averages these devations over the length of the series. Finally the RMSE will be in the same scale as the original time series so its comparison with other series is possible only if they share a common scale. RMSE has a direct connection to the L2 norm.\n\\[ \\mathrm{RMSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\sqrt{\\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2}} \\]\nParameters: y: numpy array, Actual values. y_hat: numpy array, Predicted values. mask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: rmse: numpy array, (single value)."
  },
  {
    "objectID": "losses.numpy.html#mean-absolute-percentage-error",
    "href": "losses.numpy.html#mean-absolute-percentage-error",
    "title": "NumPy Evaluation",
    "section": "Mean Absolute Percentage Error",
    "text": "Mean Absolute Percentage Error\n\nsource\n\nmape\n\n mape (y:numpy.ndarray, y_hat:numpy.ndarray,\n       weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nMean Absolute Percentage Error\nCalculates Mean Absolute Percentage Error between y and y_hat. MAPE measures the relative prediction accuracy of a forecasting method by calculating the percentual deviation of the prediction and the observed value at a given time and averages these devations over the length of the series. The closer to zero an observed value is, the higher penalty MAPE loss assigns to the corresponding error.\n\\[ \\mathrm{MAPE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{|y_{\\tau}|} \\]\nParameters: y: numpy array, Actual values. y_hat: numpy array, Predicted values. mask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: mape: numpy array, (single value)."
  },
  {
    "objectID": "losses.numpy.html#smape",
    "href": "losses.numpy.html#smape",
    "title": "NumPy Evaluation",
    "section": "SMAPE",
    "text": "SMAPE\n\nsource\n\nsmape\n\n smape (y:numpy.ndarray, y_hat:numpy.ndarray,\n        weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nSymmetric Mean Absolute Percentage Error\nCalculates Symmetric Mean Absolute Percentage Error between y and y_hat. SMAPE measures the relative prediction accuracy of a forecasting method by calculating the relative deviation of the prediction and the observed value scaled by the sum of the absolute values for the prediction and observed value at a given time, then averages these devations over the length of the series. This allows the SMAPE to have bounds between 0% and 200% which is desirable compared to normal MAPE that may be undetermined when the target is zero.\n\\[ \\mathrm{sMAPE}_{2}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{|y_{\\tau}|+|\\hat{y}_{\\tau}|} \\]\nParameters: y: numpy array, Actual values. y_hat: numpy array, Predicted values. mask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: smape: numpy array, (single value).\nReferences: Makridakis S., “Accuracy measures: theoretical and practical concerns”."
  },
  {
    "objectID": "losses.numpy.html#mean-absolute-scaled-error",
    "href": "losses.numpy.html#mean-absolute-scaled-error",
    "title": "NumPy Evaluation",
    "section": "Mean Absolute Scaled Error",
    "text": "Mean Absolute Scaled Error\n\nsource\n\nmase\n\n mase (y:numpy.ndarray, y_hat:numpy.ndarray, y_train:numpy.ndarray,\n       seasonality:int, weights:Optional[numpy.ndarray]=None,\n       axis:Optional[int]=None)\n\nMean Absolute Scaled Error Calculates the Mean Absolute Scaled Error between y and y_hat. MASE measures the relative prediction accuracy of a forecasting method by comparinng the mean absolute errors of the prediction and the observed value against the mean absolute errors of the seasonal naive model. The MASE partially composed the Overall Weighted Average (OWA), used in the M4 Competition.\n\\[ \\mathrm{MASE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{\\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau})} \\]\nParameters: y: numpy array, (batch_size, output_size), Actual values. y_hat: numpy array, (batch_size, output_size)), Predicted values. y_insample: numpy array, (batch_size, input_size), Actual insample Seasonal Naive predictions. seasonality: int. Main frequency of the time series; Hourly 24, Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1.\nmask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: mase: numpy array, (single value).\nReferences: Rob J. Hyndman, & Koehler, A. B. “Another look at measures of forecast accuracy”. Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, “The M4 Competition: 100,000 time series and 61 forecasting methods”."
  },
  {
    "objectID": "losses.numpy.html#relative-mean-absolute-error",
    "href": "losses.numpy.html#relative-mean-absolute-error",
    "title": "NumPy Evaluation",
    "section": "Relative Mean Absolute Error",
    "text": "Relative Mean Absolute Error\n\nsource\n\nrmae\n\n rmae (y:numpy.ndarray, y_hat1:numpy.ndarray, y_hat2:numpy.ndarray,\n       weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nRMAE\nCalculates Relative Mean Absolute Error (RMAE) between two sets of forecasts (from two different forecasting methods). A number smaller than one implies that the forecast in the numerator is better than the forecast in the denominator.\n\\[ \\mathrm{rMAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}^{base}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{\\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{base}_{\\tau})} \\]\nParameters: y: numpy array, observed values. y_hat1: numpy array. Predicted values of first model. y_hat2: numpy array. Predicted values of baseline model. weights: numpy array, optional. Weights for weighted average. axis: None or int, optional.Axis or axes along which to average a. The default, axis=None, will average over all of the elements of the input array.\nReturns: rmae: numpy array or double.\nReferences: Rob J. Hyndman, & Koehler, A. B. “Another look at measures of forecast accuracy”."
  },
  {
    "objectID": "losses.numpy.html#quantile-loss",
    "href": "losses.numpy.html#quantile-loss",
    "title": "NumPy Evaluation",
    "section": "Quantile Loss",
    "text": "Quantile Loss\n\nsource\n\nquantile_loss\n\n quantile_loss (y:numpy.ndarray, y_hat:numpy.ndarray, q:float=0.5,\n                weights:Optional[numpy.ndarray]=None,\n                axis:Optional[int]=None)\n\nQuantile Loss\nComputes the quantile loss between y and y_hat. QL measures the deviation of a quantile forecast. By weighting the absolute deviation in a non symmetric way, the loss pays more attention to under or over estimation. A common value for q is 0.5 for the deviation from the median (Pinball loss).\n\\[ \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q)}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\Big( (1-q)\\,( \\hat{y}^{(q)}_{\\tau} - y_{\\tau} )_{+} + q\\,( y_{\\tau} - \\hat{y}^{(q)}_{\\tau} )_{+} \\Big) \\]\nParameters: y: numpy array, Actual values. y_hat: numpy array, Predicted values. q: float, between 0 and 1. The slope of the quantile loss, in the context of quantile regression, the q determines the conditional quantile level. mask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: quantile_loss: numpy array, (single value).\nReferences: Roger Koenker and Gilbert Bassett, Jr., “Regression Quantiles”."
  },
  {
    "objectID": "losses.numpy.html#multi-quantile-loss",
    "href": "losses.numpy.html#multi-quantile-loss",
    "title": "NumPy Evaluation",
    "section": "Multi-Quantile Loss",
    "text": "Multi-Quantile Loss\n\nsource\n\nmqloss\n\n mqloss (y:numpy.ndarray, y_hat:numpy.ndarray, quantiles:numpy.ndarray,\n         weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nMulti-Quantile loss\nCalculates the Multi-Quantile loss (MQL) between y and y_hat. MQL calculates the average multi-quantile Loss for a given set of quantiles, based on the absolute difference between predicted quantiles and observed values.\n\\[ \\mathrm{MQL}(\\mathbf{y}_{\\tau},[\\mathbf{\\hat{y}}^{(q_{1})}_{\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\tau}]) = \\frac{1}{n} \\sum_{q_{i}} \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q_{i})}_{\\tau}) \\]\nThe limit behavior of MQL allows to measure the accuracy of a full predictive distribution \\(\\mathbf{\\hat{F}}_{\\tau}\\) with the continuous ranked probability score (CRPS). This can be achieved through a numerical integration technique, that discretizes the quantiles and treats the CRPS integral with a left Riemann approximation, averaging over uniformly distanced quantiles.\n\\[ \\mathrm{CRPS}(y_{\\tau}, \\mathbf{\\hat{F}}_{\\tau}) = \\int^{1}_{0} \\mathrm{QL}(y_{\\tau}, \\hat{y}^{(q)}_{\\tau}) dq \\]\nParameters: y: numpy array, Actual values. y_hat: numpy array, Predicted values. quantiles: numpy array,(n_quantiles). Quantiles to estimate from the distribution of y. mask: numpy array, Specifies date stamps per serie to consider in loss.\nReturns: mqloss: numpy array, (single value).\nReferences: Roger Koenker and Gilbert Bassett, Jr., “Regression Quantiles”. James E. Matheson and Robert L. Winkler, “Scoring Rules for Continuous Probability Distributions”."
  },
  {
    "objectID": "losses.pytorch.html",
    "href": "losses.pytorch.html",
    "title": "PyTorch Losses",
    "section": "",
    "text": "These metrics are on the same scale as the data.\n\n\n\nsource\n\n\n\n MAE.__init__ ()\n\nMean Absolute Error\nCalculates Mean Absolute Error between y and y_hat. MAE measures the relative prediction accuracy of a forecasting method by calculating the deviation of the prediction and the true value at a given time and averages these devations over the length of the series.\n\\[ \\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} |y_{\\tau} - \\hat{y}_{\\tau}| \\]\n\nsource\n\n\n\n\n MAE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n               mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: mae: tensor (single value).\n\n\n\n\n\n\nsource\n\n\n\n MSE.__init__ ()\n\nMean Squared Error\nCalculates Mean Squared Error between y and y_hat. MSE measures the relative prediction accuracy of a forecasting method by calculating the squared deviation of the prediction and the true value at a given time, and averages these devations over the length of the series.\n\\[ \\mathrm{MSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2} \\]\n\nsource\n\n\n\n\n MSE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n               mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: mse: tensor (single value).\n\n\n\n\n\n\nsource\n\n\n\n RMSE.__init__ ()\n\nRoot Mean Squared Error\nCalculates Root Mean Squared Error between y and y_hat. RMSE measures the relative prediction accuracy of a forecasting method by calculating the squared deviation of the prediction and the observed value at a given time and averages these devations over the length of the series. Finally the RMSE will be in the same scale as the original time series so its comparison with other series is possible only if they share a common scale. RMSE has a direct connection to the L2 norm.\n\\[ \\mathrm{RMSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\sqrt{\\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2}} \\]\n\nsource\n\n\n\n\n RMSE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n                mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: rmse: tensor (single value)."
  },
  {
    "objectID": "losses.pytorch.html#mean-absolute-percentage-error-mape",
    "href": "losses.pytorch.html#mean-absolute-percentage-error-mape",
    "title": "PyTorch Losses",
    "section": "Mean Absolute Percentage Error (MAPE)",
    "text": "Mean Absolute Percentage Error (MAPE)\n\nsource\n\nMAPE.__init__\n\n MAPE.__init__ ()\n\nMean Absolute Percentage Error\nCalculates Mean Absolute Percentage Error between y and y_hat. MAPE measures the relative prediction accuracy of a forecasting method by calculating the percentual deviation of the prediction and the observed value at a given time and averages these devations over the length of the series. The closer to zero an observed value is, the higher penalty MAPE loss assigns to the corresponding error.\n\\[ \\mathrm{MAPE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{|y_{\\tau}|} \\]\n\nsource\n\n\nMAPE.__call__\n\n MAPE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n                mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: mape: tensor (single value)."
  },
  {
    "objectID": "losses.pytorch.html#symmetric-mape-smape",
    "href": "losses.pytorch.html#symmetric-mape-smape",
    "title": "PyTorch Losses",
    "section": "Symmetric MAPE (sMAPE)",
    "text": "Symmetric MAPE (sMAPE)\n\nsource\n\nSMAPE.__init__\n\n SMAPE.__init__ ()\n\nSymmetric Mean Absolute Percentage Error\nCalculates Symmetric Mean Absolute Percentage Error between y and y_hat. SMAPE measures the relative prediction accuracy of a forecasting method by calculating the relative deviation of the prediction and the observed value scaled by the sum of the absolute values for the prediction and observed value at a given time, then averages these devations over the length of the series. This allows the SMAPE to have bounds between 0% and 200% which is desireble compared to normal MAPE that may be undetermined when the target is zero.\n\\[ \\mathrm{sMAPE}_{2}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{|y_{\\tau}|+|\\hat{y}_{\\tau}|} \\]\nReferences: Makridakis S., “Accuracy measures: theoretical and practical concerns”.\n\nsource\n\n\nSMAPE.__call__\n\n SMAPE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n                 mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: smape: tensor (single value)."
  },
  {
    "objectID": "losses.pytorch.html#mean-absolute-scaled-error-mase",
    "href": "losses.pytorch.html#mean-absolute-scaled-error-mase",
    "title": "PyTorch Losses",
    "section": "Mean Absolute Scaled Error (MASE)",
    "text": "Mean Absolute Scaled Error (MASE)\n\nsource\n\nMASE.__init__\n\n MASE.__init__ (seasonality:int)\n\nMean Absolute Scaled Error Calculates the Mean Absolute Scaled Error between y and y_hat. MASE measures the relative prediction accuracy of a forecasting method by comparinng the mean absolute errors of the prediction and the observed value against the mean absolute errors of the seasonal naive model. The MASE partially composed the Overall Weighted Average (OWA), used in the M4 Competition.\n\\[ \\mathrm{MASE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{\\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau})} \\]\nParameters: seasonality: int. Main frequency of the time series; Hourly 24, Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1.\nReferences: Rob J. Hyndman, & Koehler, A. B. “Another look at measures of forecast accuracy”. Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, “The M4 Competition: 100,000 time series and 61 forecasting methods”.\n\nsource\n\n\nMASE.__call__\n\n MASE.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n                y_insample:torch.Tensor, mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor (batch_size, output_size), Actual values. y_hat: tensor (batch_size, output_size)), Predicted values. y_insample: tensor (batch_size, input_size), Actual insample Seasonal Naive predictions. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: mase: tensor (single value)."
  },
  {
    "objectID": "losses.pytorch.html#quantile-loss",
    "href": "losses.pytorch.html#quantile-loss",
    "title": "PyTorch Losses",
    "section": "Quantile Loss",
    "text": "Quantile Loss\n\nsource\n\nQuantileLoss.__init__\n\n QuantileLoss.__init__ (q)\n\nQuantile Loss\nComputes the quantile loss between y and y_hat. QL measures the deviation of a quantile forecast. By weighting the absolute deviation in a non symmetric way, the loss pays more attention to under or over estimation. A common value for q is 0.5 for the deviation from the median (Pinball loss).\n\\[ \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q)}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\Big( (1-q)\\,( \\hat{y}^{(q)}_{\\tau} - y_{\\tau} )_{+} + q\\,( y_{\\tau} - \\hat{y}^{(q)}_{\\tau} )_{+} \\Big) \\]\nParameters: q: float, between 0 and 1. The slope of the quantile loss, in the context of quantile regression, the q determines the conditional quantile level.\nReferences: Roger Koenker and Gilbert Bassett, Jr., “Regression Quantiles”.\n\nsource\n\n\nQuantileLoss.__call__\n\n QuantileLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n                        mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: quantile_loss: tensor (single value)."
  },
  {
    "objectID": "losses.pytorch.html#multi-quantile-loss-mqloss",
    "href": "losses.pytorch.html#multi-quantile-loss-mqloss",
    "title": "PyTorch Losses",
    "section": "Multi Quantile Loss (MQLoss)",
    "text": "Multi Quantile Loss (MQLoss)\n\nsource\n\nMQLoss.__init__\n\n MQLoss.__init__ (level=[80, 90], quantiles=None)\n\nMulti-Quantile loss\nCalculates the Multi-Quantile loss (MQL) between y and y_hat. MQL calculates the average multi-quantile Loss for a given set of quantiles, based on the absolute difference between predicted quantiles and observed values.\n\\[ \\mathrm{MQL}(\\mathbf{y}_{\\tau},[\\mathbf{\\hat{y}}^{(q_{1})}_{\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\tau}]) = \\frac{1}{n} \\sum_{q_{i}} \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q_{i})}_{\\tau}) \\]\nThe limit behavior of MQL allows to measure the accuracy of a full predictive distribution \\(\\mathbf{\\hat{F}}_{\\tau}\\) with the continuous ranked probability score (CRPS). This can be achieved through a numerical integration technique, that discretizes the quantiles and treats the CRPS integral with a left Riemann approximation, averaging over uniformly distanced quantiles.\n\\[ \\mathrm{CRPS}(y_{\\tau}, \\mathbf{\\hat{F}}_{\\tau}) = \\int^{1}_{0} \\mathrm{QL}(y_{\\tau}, \\hat{y}^{(q)}_{\\tau}) dq \\]\nParameters: level: int list [0,100]. Probability levels for prediction intervals (Defaults median). quantiles: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\nReferences: Roger Koenker and Gilbert Bassett, Jr., “Regression Quantiles”. James E. Matheson and Robert L. Winkler, “Scoring Rules for Continuous Probability Distributions”.\n\nsource\n\n\nMQLoss.__call__\n\n MQLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n                  mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: mqloss: tensor (single value)."
  },
  {
    "objectID": "losses.pytorch.html#weighted-mqloss-wmqloss",
    "href": "losses.pytorch.html#weighted-mqloss-wmqloss",
    "title": "PyTorch Losses",
    "section": "Weighted MQLoss (wMQLoss)",
    "text": "Weighted MQLoss (wMQLoss)\n\nsource\n\nwMQLoss.__init__\n\n wMQLoss.__init__ (level=[80, 90], quantiles=None)\n\nWeighted Multi-Quantile loss\nCalculates the Weighted Multi-Quantile loss (WMQL) between y and y_hat. WMQL calculates the weighted average multi-quantile Loss for a given set of quantiles, based on the absolute difference between predicted quantiles and observed values.\n\\[ \\mathrm{wMQL}(\\mathbf{y}_{\\tau},[\\mathbf{\\hat{y}}^{(q_{1})}_{\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\tau}]) = \\frac{1}{n} \\sum_{q_{i}} \\frac{\\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q_{i})}_{\\tau})}{\\sum^{t+H}_{\\tau=t+1} |y_{\\tau}|} \\]\nParameters: level: int list [0,100]. Probability levels for prediction intervals (Defaults median). quantiles: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\nReferences: Roger Koenker and Gilbert Bassett, Jr., “Regression Quantiles”. James E. Matheson and Robert L. Winkler, “Scoring Rules for Continuous Probability Distributions”.\n\nsource\n\n\nwMQLoss.__call__\n\n wMQLoss.__call__ (y:torch.Tensor, y_hat:torch.Tensor,\n                   mask:Optional[torch.Tensor]=None)\n\nParameters: y: tensor, Actual values. y_hat: tensor, Predicted values. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns: mqloss: tensor (single value)."
  },
  {
    "objectID": "losses.pytorch.html#distributionloss",
    "href": "losses.pytorch.html#distributionloss",
    "title": "PyTorch Losses",
    "section": "DistributionLoss",
    "text": "DistributionLoss\n\nsource\n\nDistributionLoss.__init__\n\n DistributionLoss.__init__ (distribution, level=[80, 90], quantiles=None)\n\nDistributionLoss\nThis PyTorch module wraps the torch.distribution classes allowing it to interact with NeuralForecast models modularly. It shares the negative log-likelihood as the optimization objective and a sample method to generate empirically the quantiles defined by the level list.\nAdditionally, it implements a distribution transformation that factorizes the scale-dependent likelihood parameters into a base scale and a multiplier efficiently learnable within the network’s non-linearities operating ranges.\nAvailable distributions: - Poisson - Normal - StudentT\nParameters: distribution: str, identifier of a torch.distributions.Distribution class. level: float list [0,100], confidence levels for prediction intervals. quantiles: float list [0,1], alternative to level list, target quantiles.\nReferences: - PyTorch Probability Distributions Package: StudentT. - David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski (2020). “DeepAR: Probabilistic forecasting with autoregressive recurrent networks”. International Journal of Forecasting.\n\nsource\n\n\nDistributionLoss.sample\n\n DistributionLoss.sample (distr_args:torch.Tensor, loc:torch.Tensor,\n                          scale:torch.Tensor, num_samples:int=500)\n\nConstruct the empirical quantiles from the Pytorch Distribution, sampling from it num_samples independently.\nParameters distr_args: Constructor arguments for the underlying Distribution type. loc: Optional tensor, of the same shape as the batch_shape + event_shape of the resulting distribution. scale: Optional tensor, of the same shape as the batch_shape+event_shape of the resulting distribution. num_samples: int=500, number of samples for the empirical quantiles.\nReturns samples: tensor, shape [B,H,num_samples]. quantiles: tensor, empirical quantiles defined by levels.\n\nsource\n\n\nDistributionLoss.__call__\n\n DistributionLoss.__call__ (y:torch.Tensor, distr_args:torch.Tensor,\n                            loc:torch.Tensor, scale:torch.Tensor,\n                            mask:Optional[torch.Tensor]=None)\n\nComputes the negative log-likelihood objective function. To estimate the following predictive distribution:\n\\[\\mathrm{P}(\\mathbf{y}_{\\tau}\\,|\\,\\theta) \\quad \\mathrm{and} \\quad -\\log(\\mathrm{P}(\\mathbf{y}_{\\tau}\\,|\\,\\theta))\\]\nwhere \\(\\theta\\) represents the distributions parameters. It aditionally summarizes the objective signal using a weighted average using the mask tensor.\nParameters y: tensor, Actual values. distr_args: Constructor arguments for the underlying Distribution type. loc: Optional tensor, of the same shape as the batch_shape + event_shape of the resulting distribution. scale: Optional tensor, of the same shape as the batch_shape+event_shape of the resulting distribution. mask: tensor, Specifies date stamps per serie to consider in loss.\nReturns loss: scalar, weighted loss function against which backpropagation will be performed."
  },
  {
    "objectID": "losses.pytorch.html#poisson-mixture-mesh-pmm",
    "href": "losses.pytorch.html#poisson-mixture-mesh-pmm",
    "title": "PyTorch Losses",
    "section": "Poisson Mixture Mesh (PMM)",
    "text": "Poisson Mixture Mesh (PMM)\n\nsource\n\nPMM.__init__\n\n PMM.__init__ (n_components=10, level=[80, 90], quantiles=None)\n\nPoisson Mixture Mesh\nThis Poisson Mixture statistical model assumes independence across groups of data \\(\\mathcal{G}=\\{[g_{i}]\\}\\), and estimates relationships within the group.\n\\[ \\mathrm{P}\\left(\\mathbf{y}_{[b][t+1:t+H]}\\right) =\n\\prod_{ [g_{i}] \\in \\mathcal{G}} \\mathrm{P} \\left(\\mathbf{y}_{[g_{i}][\\tau]} \\right) =\n\\prod_{\\beta\\in[g_{i}]}\n\\left(\\sum_{k=1}^{K} w_k \\prod_{(\\beta,\\tau) \\in [g_i][t+1:t+H]} \\mathrm{Poisson}(y_{\\beta,\\tau}, \\hat{\\lambda}_{\\beta,\\tau,k}) \\right)\\]\nParameters: n_components: int=10, the number of mixture components. level: float list [0,100], confidence levels for prediction intervals. quantiles: float list [0,1], alternative to level list, target quantiles.\nReferences: Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. Submitted to the International Journal Forecasting, Working paper available at arxiv.\n\nsource\n\n\nPMM.sample\n\n PMM.sample (distr_args, num_samples=500, loc=None, scale=None)\n\n\nsource\n\n\nPMM.__call__\n\n PMM.__call__ (y:torch.Tensor, distr_args:Tuple[torch.Tensor],\n               mask:Optional[torch.Tensor]=None,\n               loc:Optional[torch.Tensor]=None,\n               scale:Optional[torch.Tensor]=None)\n\nCall self as a function."
  },
  {
    "objectID": "losses.pytorch.html#gaussian-mixture-mesh-gmm",
    "href": "losses.pytorch.html#gaussian-mixture-mesh-gmm",
    "title": "PyTorch Losses",
    "section": "Gaussian Mixture Mesh (GMM)",
    "text": "Gaussian Mixture Mesh (GMM)\n\nsource\n\nGMM.__init__\n\n GMM.__init__ (n_components=1, level=[80, 90], quantiles=None)\n\nGaussian Mixture Mesh\nThis Gaussian Mixture statistical model assumes independence across groups of data \\(\\mathcal{G}=\\{[g_{i}]\\}\\), and estimates relationships within the group.\n\\[ \\mathrm{P}\\left(\\mathbf{y}_{[b][t+1:t+H]}\\right) =\n\\prod_{ [g_{i}] \\in \\mathcal{G}} \\mathrm{P}\\left(\\mathbf{y}_{[g_{i}][\\tau]}\\right)=\n\\prod_{\\beta\\in[g_{i}]}\n\\left(\\sum_{k=1}^{K} w_k \\prod_{(\\beta,\\tau) \\in [g_i][t+1:t+H]}\n\\mathrm{Gaussian}(y_{\\beta,\\tau}, \\hat{\\mu}_{\\beta,\\tau,k}, \\sigma_{\\beta,\\tau,k})\\right)\\]\nParameters: n_components: int=10, the number of mixture components. level: float list [0,100], confidence levels for prediction intervals. quantiles: float list [0,1], alternative to level list, target quantiles.\nReferences: Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. Submitted to the International Journal Forecasting, Working paper available at arxiv.\n\nsource\n\n\nGMM.sample\n\n GMM.sample (weights, means, stds, num_samples=500)\n\n\nsource\n\n\nGMM.__call__\n\n GMM.__call__ (y:torch.Tensor, weights:torch.Tensor, means:torch.Tensor,\n               stds:torch.Tensor, mask:Optional[torch.Tensor]=None)\n\nCall self as a function."
  },
  {
    "objectID": "examples/uncertaintyintervals.html",
    "href": "examples/uncertaintyintervals.html",
    "title": "• Probabilistic Forecasts",
    "section": "",
    "text": "Probabilistic forecasting is a natural answer to quantify the uncertainty of target variable’s future. The task requires to model the following conditional predictive distribution:\n\\[\\mathbb{P}(\\mathbf{y}_{t+1:t+H} \\;|\\; \\mathbf{y}_{:t})\\]\nWe will show you how to tackle the task with NeuralForecast by combining a classic Long Short Term Memory Network (LSTM) and the Neural Hierarchical Interpolation (NHITS) with the multi quantile loss function (MQLoss).\n\\[ \\mathrm{MQLoss}(y_{\\tau}, [\\hat{y}^{(q1)}_{\\tau},\\hat{y}^{(q2)}_{\\tau},\\dots,\\hat{y}^{(Q)}_{\\tau}]) = \\frac{1}{H} \\sum_{q} \\mathrm{QL}(y_{\\tau}, \\hat{y}^{(q)}_{\\tau}) \\]\nIn this notebook we will: 1. Install NeuralForecast Library 2. Explore the M4-Hourly data. 3. Train the LSTM and NHITS 4. Visualize the LSTM/NHITS prediction intervals.\nYou can run these experiments using GPU with Google Colab."
  },
  {
    "objectID": "examples/uncertaintyintervals.html#installing-neuralforecast",
    "href": "examples/uncertaintyintervals.html#installing-neuralforecast",
    "title": "• Probabilistic Forecasts",
    "section": "1. Installing NeuralForecast",
    "text": "1. Installing NeuralForecast\n\n# %%capture\n# !pip install git+https://github.com/Nixtla/neuralforecast.git@main\n\n\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom itertools import product\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.losses.pytorch import MQLoss\nfrom neuralforecast.models import LSTM, DilatedRNN, NHITS\n\n\nUseful functions\nThe plot_grid auxiliary function defined below will be useful to plot different time series, and different models’ forecasts.\n\ndef plot_grid(df_train, df_test=None, plot_random=True, model=None, level=None):\n    fig, axes = plt.subplots(4, 2, figsize = (24, 14))\n\n    unique_ids = df_train['unique_id'].unique()\n\n    assert len(unique_ids) >= 8, \"Must provide at least 8 ts\"\n    \n    if plot_random:\n        unique_ids = random.sample(list(unique_ids), k=8)\n    else:\n        unique_uids = unique_ids[:8]\n\n    for uid, (idx, idy) in zip(unique_ids, product(range(4), range(2))):\n        train_uid = df_train.query('unique_id == @uid')\n        axes[idx, idy].plot(train_uid['ds'], train_uid['y'], label = 'y_train')\n        if df_test is not None:\n            max_ds = train_uid['ds'].max()\n            test_uid = df_test.query('unique_id == @uid')\n            for col in ['y', f'{model}-median', 'y_test']:\n                if col in test_uid:\n                    axes[idx, idy].plot(test_uid['ds'], test_uid[col], label=col)\n            if level is not None:\n                for l, alpha in zip(sorted(level), [0.5, .4, .35, .2]):\n                    axes[idx, idy].fill_between(\n                        test_uid['ds'], \n                        test_uid[f'{model}-lo-{l}.0'], \n                        test_uid[f'{model}-hi-{l}.0'],\n                        alpha=alpha,\n                        color='orange',\n                        label=f'{model}_level_{l}',\n                    )\n        axes[idx, idy].set_title(f'M4 Hourly: {uid}')\n        axes[idx, idy].set_xlabel('Timestamp [t]')\n        axes[idx, idy].set_ylabel('Target')\n        axes[idx, idy].legend(loc='upper left')\n        axes[idx, idy].xaxis.set_major_locator(plt.MaxNLocator(20))\n        axes[idx, idy].grid()\n    fig.subplots_adjust(hspace=0.5)\n    plt.show()"
  },
  {
    "objectID": "examples/uncertaintyintervals.html#loading-m4-data",
    "href": "examples/uncertaintyintervals.html#loading-m4-data",
    "title": "• Probabilistic Forecasts",
    "section": "2. Loading M4 Data",
    "text": "2. Loading M4 Data\nFor testing purposes, we will use the Hourly dataset from the M4 competition.\n\n!wget https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv\n!wget https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv\n\n\nY_train_df = pd.read_csv('M4-Hourly.csv')\nY_test_df = pd.read_csv('M4-Hourly-test.csv').rename(columns={'y': 'y_test'})\n\nIn this example we will use a subset of the data to avoid waiting too long. You can modify the number of series if you want.\n\nn_series = 8\nuids = Y_train_df['unique_id'].unique()[:n_series]\nY_train_df = Y_train_df.query('unique_id in @uids')\nY_test_df = Y_test_df.query('unique_id in @uids')\n\n\nplot_grid(Y_train_df, Y_test_df)"
  },
  {
    "objectID": "examples/uncertaintyintervals.html#model-training",
    "href": "examples/uncertaintyintervals.html#model-training",
    "title": "• Probabilistic Forecasts",
    "section": "3. Model Training",
    "text": "3. Model Training\nThe core.NeuralForecast provides a high-level interface with our collection of PyTorch models. NeuralForecast is instantiated with a list of models=[LSTM(...), NHITS(...)], configured for the forecasting task.\n\nThe horizon parameter controls the number of steps ahead of the predictions, in this example 48 hours ahead (2 days).\nThe MQLoss with levels=[80,90] specializes the network’s output into the 80% and 90% prediction intervals.\nThe max_epochs=500, controls the duration of the network’s training.\n\nFor more network’s instantiation details check their documentation.\n\nhorizon = 48\nlevels = [80, 90]\nmodels = [LSTM(input_size=-1, h=horizon,\n               loss=MQLoss(level=levels), max_epochs=300),\n          NHITS(input_size=7*horizon, h=horizon,\n                n_freq_downsample=[24, 12, 1],\n                loss=MQLoss(level=levels), max_epochs=200),]\nfcst = NeuralForecast(models=models, freq='H')\n\nThe models are trained using cross-learning, that is a set of correlated series in Y_train_df is used during a shared optimization.\n\nfcst.fit(df=Y_train_df)\n\n\nforecasts = fcst.predict()\nforecasts = forecasts.reset_index()\nforecasts.head()\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      LSTM-median\n      LSTM-lo-90.0\n      LSTM-lo-80.0\n      LSTM-hi-80.0\n      LSTM-hi-90.0\n      NHITS-median\n      NHITS-lo-90.0\n      NHITS-lo-80.0\n      NHITS-hi-80.0\n      NHITS-hi-90.0\n    \n  \n  \n    \n      0\n      H1\n      701\n      661.390320\n      514.213257\n      554.171082\n      772.187317\n      812.635864\n      580.240234\n      457.294403\n      519.597900\n      653.173462\n      709.418884\n    \n    \n      1\n      H1\n      702\n      608.767395\n      469.444824\n      492.870117\n      723.411987\n      763.704773\n      523.138855\n      417.413483\n      460.784607\n      590.277100\n      656.792969\n    \n    \n      2\n      H1\n      703\n      553.830139\n      408.536591\n      440.972717\n      675.927063\n      709.604065\n      481.189453\n      377.372284\n      398.614319\n      566.097351\n      602.510559\n    \n    \n      3\n      H1\n      704\n      509.802368\n      359.942810\n      386.292145\n      633.949219\n      680.044678\n      451.847473\n      353.026123\n      376.023682\n      536.535156\n      573.252197\n    \n    \n      4\n      H1\n      705\n      479.271240\n      321.798004\n      357.680634\n      608.942566\n      648.161865\n      429.752136\n      316.402679\n      368.250336\n      510.772217\n      576.168213\n    \n  \n\n\n\n\n\nY_test_df = Y_test_df.merge(forecasts, how='left', on=['unique_id', 'ds'])"
  },
  {
    "objectID": "examples/uncertaintyintervals.html#plotting-predictions",
    "href": "examples/uncertaintyintervals.html#plotting-predictions",
    "title": "• Probabilistic Forecasts",
    "section": "4. Plotting Predictions",
    "text": "4. Plotting Predictions\nHere we finalize our analysis by plotting the prediction intervals and verifying that both the LSTM and NHITS are giving reasonable results.\nConsider the output [NHITS-lo-90.0, NHITS-hi-90.0], that represents the 80% prediction interval of the NHITS network; its lower limit gives the 5th percentile (or 0.05 quantile) while its upper limit gives the 95th percentile (or 0.95 quantile). For well-trained models we expect that the target values lie within the interval 90% of the time.\n\nLSTM\n\nplot_grid(Y_train_df, Y_test_df, level=levels, model='LSTM')\n\n\n\n\n\n\nNHITS\n\nplot_grid(Y_train_df, Y_test_df, level=levels, model='NHITS')"
  },
  {
    "objectID": "examples/uncertaintyintervals.html#references",
    "href": "examples/uncertaintyintervals.html#references",
    "title": "• Probabilistic Forecasts",
    "section": "References",
    "text": "References\n\nRoger Koenker and Gilbert Basset (1978). Regression Quantiles, Econometrica.\nJeffrey L. Elman (1990). “Finding Structure in Time”.\nCristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2022). “N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting”."
  },
  {
    "objectID": "examples/longhorizon_with_nhits.html",
    "href": "examples/longhorizon_with_nhits.html",
    "title": "• Long-Horizon Forecast",
    "section": "",
    "text": "Long-horizon forecasting is challenging because of the volatility of the predictions and the computational complexity. To solve this problem we created the NHITS model and made the code available NeuralForecast library. NHITS specializes its partial outputs in the different frequencies of the time series through hierarchical interpolation and multi-rate input processing.\nIn this notebook we show how to use N-HiTS on the ETTm2 benchmark dataset. This data set includes data points for 2 Electricity Transformers at 2 stations, including load, oil temperature.\nWe will show you how to load data, train, and perform automatic hyperparameter tuning, to achieve SoTA performance, outperforming even the latest Transformer architectures for a fraction of their computational cost (50x faster).\nYou can run these experiments using GPU with Google Colab."
  },
  {
    "objectID": "examples/longhorizon_with_nhits.html#installing-neuralforecast",
    "href": "examples/longhorizon_with_nhits.html#installing-neuralforecast",
    "title": "• Long-Horizon Forecast",
    "section": "1. Installing NeuralForecast",
    "text": "1. Installing NeuralForecast\n\n!pip install neuralforecast datasetsforecast\n\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom ray import tune\n\nfrom neuralforecast.auto import AutoNHITS\nfrom neuralforecast.core import NeuralForecast\n\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.losses.numpy import mae, mse\nfrom datasetsforecast.long_horizon import LongHorizon\n\n\nimport logging\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n\nThis example will automatically run on GPUs if available. Make sure cuda is available. (If you need help to put this into production send us an email or join or community, we also offer a fully hosted solution)\n\ntorch.cuda.is_available()\n\nTrue"
  },
  {
    "objectID": "examples/longhorizon_with_nhits.html#load-ettm2-data",
    "href": "examples/longhorizon_with_nhits.html#load-ettm2-data",
    "title": "• Long-Horizon Forecast",
    "section": "2. Load ETTm2 Data",
    "text": "2. Load ETTm2 Data\nThe LongHorizon class will automatically download the complete ETTm2 dataset and process it.\nIt return three Dataframes: Y_df contains the values for the target variables, X_df contains exogenous calendar features and S_df contains static features for each time-series (none for ETTm2). For this example we will only use Y_df.\nIf you want to use your own data just replace Y_df. Be sure to use a long format and have a simmilar structure than our data set.\n\n# Change this to your own data to try the model\nY_df, _, _ = LongHorizon.load(directory='./', group='ETTm2')\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\n# For this excercise we are going to take 20% of the DataSet\nn_time = len(Y_df.ds.unique())\nval_size = int(.2 * n_time)\ntest_size = int(.2 * n_time)\n\nY_df.groupby('unique_id').head(2)\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      HUFL\n      2016-07-01 00:00:00\n      -0.041413\n    \n    \n      1\n      HUFL\n      2016-07-01 00:15:00\n      -0.185467\n    \n    \n      57600\n      HULL\n      2016-07-01 00:00:00\n      0.040104\n    \n    \n      57601\n      HULL\n      2016-07-01 00:15:00\n      -0.214450\n    \n    \n      115200\n      LUFL\n      2016-07-01 00:00:00\n      0.695804\n    \n    \n      115201\n      LUFL\n      2016-07-01 00:15:00\n      0.434685\n    \n    \n      172800\n      LULL\n      2016-07-01 00:00:00\n      0.434430\n    \n    \n      172801\n      LULL\n      2016-07-01 00:15:00\n      0.428168\n    \n    \n      230400\n      MUFL\n      2016-07-01 00:00:00\n      -0.599211\n    \n    \n      230401\n      MUFL\n      2016-07-01 00:15:00\n      -0.658068\n    \n    \n      288000\n      MULL\n      2016-07-01 00:00:00\n      -0.393536\n    \n    \n      288001\n      MULL\n      2016-07-01 00:15:00\n      -0.659338\n    \n    \n      345600\n      OT\n      2016-07-01 00:00:00\n      1.018032\n    \n    \n      345601\n      OT\n      2016-07-01 00:15:00\n      0.980124\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# We are going to plot the temperature of the transformer \n# and marking the validation and train splits\nu_id = 'HUFL'\nx_plot = pd.to_datetime(Y_df[Y_df.unique_id==u_id].ds)\ny_plot = Y_df[Y_df.unique_id==u_id].y.values\n\nx_val = x_plot[n_time - val_size - test_size]\nx_test = x_plot[n_time - test_size]\n\nfig = plt.figure(figsize=(10, 5))\nfig.tight_layout()\n\nplt.plot(x_plot, y_plot)\nplt.xlabel('Date', fontsize=17)\nplt.ylabel('HUFL [15 min temperature]', fontsize=17)\n\nplt.axvline(x_val, color='black', linestyle='-.')\nplt.axvline(x_test, color='black', linestyle='-.')\nplt.text(x_val, 5, '  Validation', fontsize=12)\nplt.text(x_test, 5, '  Test', fontsize=12)\n\nplt.grid()\nplt.show()\nplt.close()\n\n\n\n\n\nY_df.unique_id.unique()\n\narray(['HUFL', 'HULL', 'LUFL', 'LULL', 'MUFL', 'MULL', 'OT'], dtype=object)"
  },
  {
    "objectID": "examples/longhorizon_with_nhits.html#define-hyperparameter-space",
    "href": "examples/longhorizon_with_nhits.html#define-hyperparameter-space",
    "title": "• Long-Horizon Forecast",
    "section": "3. Define Hyperparameter Space",
    "text": "3. Define Hyperparameter Space\nThe AutoNHITS class contains a pre-defined suggested hyperparameter space, built for the Hyperopt library. This function only needs dataframe specific information such as the number of series and frequency. Feel free to play around with this space.\nThe parameter h is used to specify the desired forecasting horizon. To replicate results for other horizons from the paper, just change this value!\nThe AutoNHITS.default_config attribute contains a suggested hyperparameter space. Here we send the paper’s config.\n\nhorizon = 96 # 24hrs = 4 * 15 min.\n\n# Use your own config or AutoNHITS.default_config\nnhits_config = {\n       \"learning_rate\": tune.choice([1e-3]),                                     # Initial Learning rate\n       \"max_steps\": tune.choice([1000]),                                         # Number of SGD steps\n       \"input_size\": tune.choice([5 * horizon]),                                 # input_size = multiplier * horizon\n       \"batch_size\": tune.choice([7]),                                           # Number of series in windows\n       \"windows_batch_size\": tune.choice([256]),                                 # Number of windows in batch\n       \"n_pool_kernel_size\": tune.choice([[2, 2, 2], [16, 8, 1]]),               # MaxPool's Kernelsize\n       \"n_freq_downsample\": tune.choice([[168, 24, 1], [24, 12, 1], [1, 1, 1]]), # Interpolation expressivity ratios\n       \"activation\": tune.choice(['ReLU']),                                      # Type of non-linear activation\n       \"n_blocks\":  tune.choice([[1, 1, 1]]),                                    # Blocks per each 3 stacks\n       \"mlp_units\":  tune.choice([[[512, 512], [512, 512], [512, 512]]]),        # 2 512-Layers per block for each stack\n       \"interpolation_mode\": tune.choice(['linear']),                            # Type of multi-step interpolation\n       \"random_seed\": tune.randint(1, 10),\n    }\n\nYou can override the NHiTS config for each hyperparameter.\nNotice that 1000 Stochastic Gradient Steps are enough to achieve SoTA performance.\nRefer to http://hyperopt.github.io/hyperopt/ for more information on the different space options, such as lists and continous intervals.m"
  },
  {
    "objectID": "examples/longhorizon_with_nhits.html#hyperparameter-tuning",
    "href": "examples/longhorizon_with_nhits.html#hyperparameter-tuning",
    "title": "• Long-Horizon Forecast",
    "section": "4. Hyperparameter Tuning",
    "text": "4. Hyperparameter Tuning\nThe function hyperopt_tunning will perform the automatic hyperparameter optimization using the Hyperopt library for any of our implemented models, on the specified space, on any dataframes Y_df and with custom validation and test losses.\nThis function will split the data in Y_df based on the number of timestamps for the validation and test sets, specified with ds_in_val and ds_in_test. Each configuration will be trained on the train split, evaluated on the validation and test sets with the desired loss functions.\nUse the hyperopt_max_evals parameter to change the number of configurations explored (5 is enough for achieving SoTA performance, but more iterations will further improve results).\n\n# Fit and predict\nfcst = NeuralForecast(\n    models=[AutoNHITS(h=horizon, config=nhits_config, \n                      num_samples=5)], # control of hyperopt samples\n    freq='15min')\n\nfcst_df = fcst.cross_validation(df=Y_df, val_size=val_size,\n                                test_size=test_size, n_windows=None)"
  },
  {
    "objectID": "examples/longhorizon_with_nhits.html#evaluate-results",
    "href": "examples/longhorizon_with_nhits.html#evaluate-results",
    "title": "• Long-Horizon Forecast",
    "section": "5. Evaluate Results",
    "text": "5. Evaluate Results\nThe AutoNHITS class contains a .results tune attribute. For each configuration explored, with information of the hyperparameter optimization. It contains the validation loss, and best validation hyperparameter.\n\nfcst.models[0].results.get_best_result().config\n\n{'learning_rate': 0.001,\n 'max_steps': 1000,\n 'input_size': 480,\n 'batch_size': 7,\n 'windows_batch_size': 256,\n 'n_pool_kernel_size': [2, 2, 2],\n 'n_freq_downsample': [168, 24, 1],\n 'activation': 'ReLU',\n 'n_blocks': [1, 1, 1],\n 'mlp_units': [[512, 512], [512, 512], [512, 512]],\n 'interpolation_mode': 'linear',\n 'random_seed': 1,\n 'h': 96}\n\n\n\ny_true = fcst_df.y.values\ny_hat = fcst_df['AutoNHITS'].values\n\nn_series = len(Y_df.unique_id.unique())\n\ny_true = y_true.reshape(n_series, -1, horizon)\ny_hat = y_hat.reshape(n_series, -1, horizon)\n\nprint('Parsed results')\nprint('2. y_true.shape (n_series, n_windows, n_time_out):\\t', y_true.shape)\nprint('2. y_hat.shape  (n_series, n_windows, n_time_out):\\t', y_hat.shape)\n\nParsed results\n2. y_true.shape (n_series, n_windows, n_time_out):   (7, 11425, 96)\n2. y_hat.shape  (n_series, n_windows, n_time_out):   (7, 11425, 96)\n\n\n\nfig, axs = plt.subplots(nrows=3, ncols=1, figsize=(10, 11))\nfig.tight_layout()\n\nseries = ['HUFL','HULL','LUFL','LULL','MUFL','MULL','OT']\nseries_idx = 3\n\nfor idx, w_idx in enumerate([200, 300, 400]):\n  axs[idx].plot(y_true[series_idx, w_idx,:],label='True')\n  axs[idx].plot(y_hat[series_idx, w_idx,:],label='Forecast')\n  axs[idx].grid()\n  axs[idx].set_ylabel(series[series_idx]+f' window {w_idx}', \n                      fontsize=17)\n  if idx==2:\n    axs[idx].set_xlabel('Forecast Horizon', fontsize=17)\nplt.legend()\nplt.show()\nplt.close()\n\n\n\n\nFinally, we compute the test errors for the two metrics of interest (which are also available in the trials object):\n\\(\\qquad MAE = \\frac{1}{Windows * Horizon} \\sum_{\\tau} |y_{\\tau} - \\hat{y}_{\\tau}| \\qquad\\) and \\(\\qquad MSE = \\frac{1}{Windows * Horizon} \\sum_{\\tau} (y_{\\tau} - \\hat{y}_{\\tau})^{2} \\qquad\\)\n\nprint('MAE: ', mae(y_hat, y_true))\nprint('MSE: ', mse(y_hat, y_true))\n\nMAE:  0.264508916686362\nMSE:  0.19353899829479543\n\n\nFor reference we can check the performance when compared to previous ‘state-of-the-art’ long-horizon Transformer-based forecasting methods from the N-HiTS paper. To recover or improve the paper results try setting hyperopt_max_evals=30 in Hyperparameter Tuning.\nMean Absolute Error (MAE):\n\n\n\nHorizon\nN-HiTS\nAutoFormer\nInFormer\nARIMA\n\n\n\n\n96\n0.255\n0.339\n0.453\n0.301\n\n\n192\n0.305\n0.340\n0.563\n0.345\n\n\n336\n0.346\n0.372\n0.887\n0.386\n\n\n720\n0.426\n0.419\n1.388\n0.445\n\n\n\nMean Squared Error (MSE):\n\n\n\nHorizon\nN-HiTS\nAutoFormer\nInFormer\nARIMA\n\n\n\n\n96\n0.176\n0.255\n0.365\n0.225\n\n\n192\n0.245\n0.281\n0.533\n0.298\n\n\n336\n0.295\n0.339\n1.363\n0.370\n\n\n720\n0.401\n0.422\n3.379\n0.478"
  },
  {
    "objectID": "examples/save_load_models.html",
    "href": "examples/save_load_models.html",
    "title": "• Save and Load Models",
    "section": "",
    "text": "In this notebook we give examples regarding the saving and loading of NeuralForecast models.\nThe two methods to consider are: 1. NeuralForecast.save: Saves models into disk, allows save dataset and config. 2. NeuralForecast.load: Loads models from a given path.\nYou can run these experiments using GPU with Google Colab."
  },
  {
    "objectID": "examples/save_load_models.html#installing-neuralforecast",
    "href": "examples/save_load_models.html#installing-neuralforecast",
    "title": "• Save and Load Models",
    "section": "1. Installing NeuralForecast",
    "text": "1. Installing NeuralForecast\n\n!pip install git+https://github.com/nixtla/neuralforecast.git\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, Markdown\n\nfrom ray import tune\n\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.auto import AutoMLP\nfrom neuralforecast.models import NBEATS, NHITS\nfrom neuralforecast.utils import AirPassengersDF"
  },
  {
    "objectID": "examples/save_load_models.html#loading-airpassengers-data",
    "href": "examples/save_load_models.html#loading-airpassengers-data",
    "title": "• Save and Load Models",
    "section": "2. Loading AirPassengers Data",
    "text": "2. Loading AirPassengers Data\n\nY_df = AirPassengersDF\nY_df = Y_df.reset_index(drop=True)\nY_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      1.0\n      1949-01-31\n      112.0\n    \n    \n      1\n      1.0\n      1949-02-28\n      118.0\n    \n    \n      2\n      1.0\n      1949-03-31\n      132.0\n    \n    \n      3\n      1.0\n      1949-04-30\n      129.0\n    \n    \n      4\n      1.0\n      1949-05-31\n      121.0"
  },
  {
    "objectID": "examples/save_load_models.html#model-training",
    "href": "examples/save_load_models.html#model-training",
    "title": "• Save and Load Models",
    "section": "3. Model Training",
    "text": "3. Model Training\n\n# Split train/test sets\nY_train_df = Y_df[Y_df.ds<='1959-12-31'] # 132 train\nY_test_df = Y_df[Y_df.ds>'1959-12-31']   # 12 test\n\n\nhorizon = len(Y_test_df)\nmodels = [NBEATS(input_size=2 * horizon, h=horizon, max_epochs=50),\n          NBEATS(input_size=3 * horizon, h=horizon, max_epochs=50),\n          NHITS(input_size=2 * horizon, h=horizon, max_epochs=50),\n          AutoMLP(# Ray tune explore config\n                  config=dict(max_steps=100, # Operates with steps not epochs\n                              input_size=tune.choice([3*horizon]),\n                              learning_rate=tune.choice([1e-3])),\n                  h=12,\n                  num_samples=1, cpus=1)]\nfcst = NeuralForecast(models=models, freq='M')\n\n\nfcst.fit(df=Y_train_df)\n\n\nY_hat_df = fcst.predict().reset_index()\nY_hat_df = Y_test_df.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])\nplot_df = pd.concat([Y_train_df, Y_hat_df]).set_index('ds')\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nplot_df[['y', 'NBEATS', 'NBEATS1', 'NHITS', 'AutoMLP']].plot(ax=ax, linewidth=2)\n\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()"
  },
  {
    "objectID": "examples/save_load_models.html#save-and-load-models",
    "href": "examples/save_load_models.html#save-and-load-models",
    "title": "• Save and Load Models",
    "section": "4. Save and Load Models",
    "text": "4. Save and Load Models\n\nfcst.save(path='./checkpoints/test_run/', model_index=None, \n          overwrite=True, save_dataset=True)\n\n\nfcst2 = NeuralForecast.load(path='./checkpoints/test_run/')\n\n\nY_hat_df = fcst2.predict().reset_index()\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nY_hat_df = Y_test_df.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])\nplot_df = pd.concat([Y_train_df, Y_hat_df]).set_index('ds')\n\nplot_df[['y', 'NBEATS', 'NBEATS1', 'NHITS', 'MLP']].plot(ax=ax, linewidth=2)\n\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()"
  },
  {
    "objectID": "examples/getting_started.html",
    "href": "examples/getting_started.html",
    "title": "• Getting Started",
    "section": "",
    "text": "This notebook provides an example on how to start using the main functionalities of the NeuralForecast library. The NeuralForecast class allows users to easily interact with NeuralForecast.models PyTorch models. In this example we will forecast AirPassengers data with a classic LSTM and the recent NHITS models. The full list of available models is available here.\nYou can run these experiments using GPU with Google Colab."
  },
  {
    "objectID": "examples/getting_started.html#installing-neuralforecast",
    "href": "examples/getting_started.html#installing-neuralforecast",
    "title": "• Getting Started",
    "section": "1. Installing NeuralForecast",
    "text": "1. Installing NeuralForecast\n\n!pip install neuralforecast\n\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display, Markdown\n\nimport matplotlib.pyplot as plt\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import LSTM, NHITS, RNN\nfrom neuralforecast.utils import AirPassengersDF"
  },
  {
    "objectID": "examples/getting_started.html#loading-airpassengers-data",
    "href": "examples/getting_started.html#loading-airpassengers-data",
    "title": "• Getting Started",
    "section": "2. Loading AirPassengers Data",
    "text": "2. Loading AirPassengers Data\nThe core.NeuralForecast class contains shared, fit, predict and other methods that take as inputs pandas DataFrames with columns ['unique_id', 'ds', 'y'], where unique_id identifies individual time series from the dataset, ds is the date, and y is the target variable.\nIn this example dataset consists of a set of a single series, but you can easily fit your model to larger datasets in long format.\n\nY_df = AirPassengersDF # Defined in neuralforecast.utils\nY_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      1.0\n      1949-01-31\n      112.0\n    \n    \n      1\n      1.0\n      1949-02-28\n      118.0\n    \n    \n      2\n      1.0\n      1949-03-31\n      132.0\n    \n    \n      3\n      1.0\n      1949-04-30\n      129.0\n    \n    \n      4\n      1.0\n      1949-05-31\n      121.0\n    \n  \n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDataFrames must include all ['unique_id', 'ds', 'y'] columns. Make sure y column does not have missing or non-numeric values."
  },
  {
    "objectID": "examples/getting_started.html#model-training",
    "href": "examples/getting_started.html#model-training",
    "title": "• Getting Started",
    "section": "3. Model Training",
    "text": "3. Model Training\n\nFit the models\nUsing the NeuralForecast.fit method you can train a set of models to your dataset. You can define the forecasting horizon (12 in this example), and modify the hyperparameters of the model. For example, for the LSTM we changed the default hidden size for both encoder and decoders.\n\nhorizon = 12\n\n# Try different hyperparmeters to improve accuracy.\nmodels = [LSTM(h=horizon,                    # Forecast horizon\n               max_epochs=500,               # Number of epochs to train\n               scaler_type='standard',       # Type of scaler to normalize data\n               encoder_hidden_size=64,       # Defines the size of the hidden state of the LSTM\n               decoder_hidden_size=64,),     # Defines the number of hidden units of each layer of the MLP decoder\n          NHITS(h=horizon,                   # Forecast horizon\n                input_size=2 * horizon,      # Length of input sequence\n                max_epochs=100,              # Number of epochs to train\n                n_freq_downsample=[2, 1, 1]) # Downsampling factors for each stack output\n          ]\nfcst = NeuralForecast(models=models, freq='M')\nfcst.fit(df=Y_df)\n\n\n\n\n\n\n\nTip\n\n\n\nThe performance of Deep Learning models can be very sensitive to the choice of hyperparameters. Tuning the correct hyperparameters is an important step to obtain the best forecasts. The Auto version of these models, AutoLSTM and AutoNHITS, already perform hyperparameter selection automatically.\n\n\n\n\nPredict using the fitted models\nUsing the NeuralForecast.predict method you can obtain the h forecasts after the training data Y_df.\n\nY_hat_df = fcst.predict()\n\nPredicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 50.58it/s]\nPredicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 126.52it/s]\n\n\nThe NeuralForecast.predict method returns a DataFrame with the forecasts for each unique_id, ds, and model.\n\nY_hat_df = Y_hat_df.reset_index()\nY_hat_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      LSTM\n      NHITS\n    \n  \n  \n    \n      0\n      1.0\n      1961-01-31\n      424.380310\n      453.039185\n    \n    \n      1\n      1.0\n      1961-02-28\n      442.092010\n      429.609192\n    \n    \n      2\n      1.0\n      1961-03-31\n      448.555664\n      498.796204\n    \n    \n      3\n      1.0\n      1961-04-30\n      473.586609\n      509.536224\n    \n    \n      4\n      1.0\n      1961-05-31\n      512.466370\n      524.131592"
  },
  {
    "objectID": "examples/getting_started.html#plot-predictions",
    "href": "examples/getting_started.html#plot-predictions",
    "title": "• Getting Started",
    "section": "4. Plot Predictions",
    "text": "4. Plot Predictions\nFinally, we plot the forecasts of both models againts the real values.\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nplot_df = pd.concat([Y_df, Y_hat_df]).set_index('ds') # Concatenate the train and forecast dataframes\nplot_df[['y', 'LSTM', 'NHITS']].plot(ax=ax, linewidth=2)\n\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor this guide we are using a simple LSTM model. More recent models, such as RNN, GRU, and DilatedRNN achieve better accuracy than LSTM in most settings. The full list of available models is available here."
  },
  {
    "objectID": "examples/getting_started.html#references",
    "href": "examples/getting_started.html#references",
    "title": "• Getting Started",
    "section": "References",
    "text": "References\n\nBoris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio (2020). “N-BEATS: Neural basis expansion analysis for interpretable time series forecasting”. International Conference on Learning Representations.\nCristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). “N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting”. Work in progress paper, submitted to AAAI."
  },
  {
    "objectID": "examples/data_format.html",
    "href": "examples/data_format.html",
    "title": "• NeuralForecast Data Inputs",
    "section": "",
    "text": "In this example we will go through the dataset input requirements of the core.NeuralForecast class.\nThe core.NeuralForecast methods operate as global models that receive a set of time series rather than single series. The class uses cross-learning technique to fit flexible-shared models such as neural networks improving its generalization capabilities as shown by the M4 international forecasting competition (Smyl 2019, Semenoglou 2021).\nYou can run these experiments using GPU with Google Colab."
  },
  {
    "objectID": "examples/data_format.html#long-format",
    "href": "examples/data_format.html#long-format",
    "title": "• NeuralForecast Data Inputs",
    "section": "Long format",
    "text": "Long format\n\nMultiple time series\nStore your time series in a pandas dataframe in long format, that is, each row represents an observation for a specific series and timestamp. Let’s see an example using the datasetsforecast library.\nY_df = pd.concat( [series1, series2, ...])\n\n!pip install datasetsforecast\n\n\nimport pandas as pd\nfrom datasetsforecast.m3 import M3\n\n\nY_df, *_ = M3.load('./data', group='Yearly')\n\n100%|██████████| 1.76M/1.76M [00:00<00:00, 5.55MiB/s]\nINFO:datasetsforecast.utils:Successfully downloaded M3C.xls, 1757696, bytes.\n\n\n\nY_df.groupby('unique_id').head(2)\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      Y1\n      1975-12-31\n      940.66\n    \n    \n      1\n      Y1\n      1976-12-31\n      1084.86\n    \n    \n      20\n      Y10\n      1975-12-31\n      2160.04\n    \n    \n      21\n      Y10\n      1976-12-31\n      2553.48\n    \n    \n      40\n      Y100\n      1975-12-31\n      1424.70\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      18260\n      Y97\n      1976-12-31\n      1618.91\n    \n    \n      18279\n      Y98\n      1975-12-31\n      1164.97\n    \n    \n      18280\n      Y98\n      1976-12-31\n      1277.87\n    \n    \n      18299\n      Y99\n      1975-12-31\n      1870.00\n    \n    \n      18300\n      Y99\n      1976-12-31\n      1307.20\n    \n  \n\n1290 rows × 3 columns\n\n\n\n\nY_df.groupby('unique_id').tail(2)\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      18\n      Y1\n      1993-12-31\n      8407.84\n    \n    \n      19\n      Y1\n      1994-12-31\n      9156.01\n    \n    \n      38\n      Y10\n      1993-12-31\n      3187.00\n    \n    \n      39\n      Y10\n      1994-12-31\n      3058.00\n    \n    \n      58\n      Y100\n      1993-12-31\n      3539.00\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      18278\n      Y97\n      1994-12-31\n      4507.00\n    \n    \n      18297\n      Y98\n      1993-12-31\n      1801.00\n    \n    \n      18298\n      Y98\n      1994-12-31\n      1710.00\n    \n    \n      18317\n      Y99\n      1993-12-31\n      2379.30\n    \n    \n      18318\n      Y99\n      1994-12-31\n      2723.00\n    \n  \n\n1290 rows × 3 columns\n\n\n\nY_df is a dataframe with three columns: unique_id with a unique identifier for each time series, a column ds with the datestamp and a column y with the values of the series.\n\n\nSingle time series\nIf you have only one time series, you have to include the unique_id column. Consider, for example, the AirPassengers dataset.\n\nY_df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/air_passengers.csv')\n\nIn this example Y_df only contains two columns: timestamp, and value. To use NeuralForecast we have to include the unique_id column and rename the previuos ones.\n\nY_df['unique_id'] = 1. # We can add an integer as identifier\nY_df = Y_df.rename(columns={'timestamp': 'ds', 'value': 'y'})\nY_df = Y_df[['unique_id', 'ds', 'y']]\n\n\nY_df\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      1.0\n      1949-01-01\n      112\n    \n    \n      1\n      1.0\n      1949-02-01\n      118\n    \n    \n      2\n      1.0\n      1949-03-01\n      132\n    \n    \n      3\n      1.0\n      1949-04-01\n      129\n    \n    \n      4\n      1.0\n      1949-05-01\n      121\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      139\n      1.0\n      1960-08-01\n      606\n    \n    \n      140\n      1.0\n      1960-09-01\n      508\n    \n    \n      141\n      1.0\n      1960-10-01\n      461\n    \n    \n      142\n      1.0\n      1960-11-01\n      390\n    \n    \n      143\n      1.0\n      1960-12-01\n      432\n    \n  \n\n144 rows × 3 columns"
  },
  {
    "objectID": "examples/data_format.html#references",
    "href": "examples/data_format.html#references",
    "title": "• NeuralForecast Data Inputs",
    "section": "References",
    "text": "References\n\nSlawek Smyl. (2019). “A hybrid method of exponential smoothing and recurrent networks for time series forecasting”. International Journal of Forecasting.\nArtemios-Anargyros Semenoglou, Evangelos Spiliotis, Spyros Makridakis, and Vassilios Assimakopoulos. (2021). Investigating the accuracy of cross-learning time series forecasting methods”. International Journal of Forecasting."
  },
  {
    "objectID": "examples/transfer_learning.html",
    "href": "examples/transfer_learning.html",
    "title": "• Transfer Learning",
    "section": "",
    "text": "Transfer learning refers to the process of pre-training a flexible model on a large dataset and using it later on other data with little to no training. It is one of the most outstanding 🚀 achievements in Machine Learning 🧠 and has many practical applications.\nFor time series forecasting, the technique allows you to get lightning-fast predictions ⚡ bypassing the tradeoff between accuracy and speed (more than 30 times faster than our alreadsy fast autoARIMA for a similar accuracy).\nThis notebook shows how to generate a pre-trained model and store it in a checkpoint to make it available to forecast new time series never seen by the model.\nTable of Contents 1. Installing NeuralForecast/DatasetsForecast 2. Load M4 Data 3. Instantiate NeuralForecast core, Fit, and save 4. Load pre-trained model and predict on AirPassengers 5. Evaluate Results\nYou can run these experiments using GPU with Google Colab."
  },
  {
    "objectID": "examples/transfer_learning.html#installing-libraries",
    "href": "examples/transfer_learning.html#installing-libraries",
    "title": "• Transfer Learning",
    "section": "1. Installing Libraries",
    "text": "1. Installing Libraries\n\n# %%capture\n# !pip install git+https://github.com/Nixtla/datasetsforecast.git@main\n\n\n# %%capture\n# !pip install neuralforecast\n\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom IPython.display import display, Markdown\n\nimport matplotlib.pyplot as plt\n\nfrom datasetsforecast.m4 import M4\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.models import NHITS\nfrom neuralforecast.utils import AirPassengersDF\nfrom neuralforecast.losses.numpy import mae, mse\n\n\nimport logging\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n\nThis example will automatically run on GPUs if available. Make sure cuda is available. (If you need help to put this into production send us an email or join or community, we also offer a fully hosted solution)\n\ntorch.cuda.is_available()"
  },
  {
    "objectID": "examples/transfer_learning.html#load-m4-data",
    "href": "examples/transfer_learning.html#load-m4-data",
    "title": "• Transfer Learning",
    "section": "2. Load M4 Data",
    "text": "2. Load M4 Data\nThe M4 class will automatically download the complete M4 dataset and process it.\nIt return three Dataframes: Y_df contains the values for the target variables, X_df contains exogenous calendar features and S_df contains static features for each time-series (none for M4). For this example we will only use Y_df.\nIf you want to use your own data just replace Y_df. Be sure to use a long format and have a simmilar structure than our data set.\n\nY_df, _, _ = M4.load(directory='./', group='Monthly', cache=True)\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\nY_df"
  },
  {
    "objectID": "examples/transfer_learning.html#model-train-and-save",
    "href": "examples/transfer_learning.html#model-train-and-save",
    "title": "• Transfer Learning",
    "section": "3. Model Train and Save",
    "text": "3. Model Train and Save\nUsing the NeuralForecast.fit method you can train a set of models to your dataset. You just have to define the input_size and horizon of your model. The input_size is the number of historic observations (lags) that the model will use to learn to predict h steps in the future. Also, you can modify the hyperparameters of the model to get a better accuracy.\n\nhorizon = 12\nstacks = 3\nmodels = [NHITS(input_size=5 * horizon,\n                h=horizon,\n                max_steps=100,\n                stack_types = stacks*['identity'],\n                n_blocks = stacks*[1],\n                mlp_units = [[256,256] for _ in range(stacks)],\n                n_pool_kernel_size = stacks*[1],\n                batch_size = 32,\n                scaler_type='standard',\n                n_freq_downsample=[12,4,1])]\nfcst = NeuralForecast(models=models, freq='M')\nfcst.fit(df=Y_df)\n\nSave model with core.NeuralForecast.save method. This method uses PytorchLightning save_checkpoint function. We set save_dataset=False to only save the model.\n\nfcst.save(path='./results/transfer/', model_index=None, overwrite=True, save_dataset=False)"
  },
  {
    "objectID": "examples/transfer_learning.html#transfer-m4-to-airpassengers",
    "href": "examples/transfer_learning.html#transfer-m4-to-airpassengers",
    "title": "• Transfer Learning",
    "section": "4. Transfer M4 to AirPassengers",
    "text": "4. Transfer M4 to AirPassengers\nWe load the stored model with the core.NeuralForecast.load method, and forecast AirPassenger with the core.NeuralForecast.predict function.\n\nfcst2 = NeuralForecast.load(path='./results/transfer/')\n\n\n# We define the train df. \nY_df = AirPassengersDF.copy()\nmean = Y_df[Y_df.ds<='1959-12-31']['y'].mean()\nstd = Y_df[Y_df.ds<='1959-12-31']['y'].std()\n\nY_train_df = Y_df[Y_df.ds<='1959-12-31'] # 132 train\nY_test_df = Y_df[Y_df.ds>'1959-12-31']   # 12 test\n\n\nY_hat_df = fcst2.predict(df=Y_train_df).reset_index()\nY_hat_df.head()\n\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nY_hat_df = Y_test_df.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])\nplot_df = pd.concat([Y_train_df, Y_hat_df]).set_index('ds')\n\nplot_df[['y', 'NHITS']].plot(ax=ax, linewidth=2)\n\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()"
  },
  {
    "objectID": "examples/transfer_learning.html#evaluate-results",
    "href": "examples/transfer_learning.html#evaluate-results",
    "title": "• Transfer Learning",
    "section": "5. Evaluate Results",
    "text": "5. Evaluate Results\nWe evaluate the forecasts of the pre-trained model with the Mean Absolute Error (mae).\n\\[\n\\qquad MAE = \\frac{1}{Horizon} \\sum_{\\tau} |y_{\\tau} - \\hat{y}_{\\tau}|\\qquad\n\\]\n\ny_true = Y_test_df.y.values\ny_hat = Y_hat_df['NHITS'].values\n\n\nprint('NHITS     MAE: %0.3f' % mae(y_hat, y_true))\nprint('ETS       MAE: 16.222')\nprint('AutoARIMA MAE: 18.551')"
  },
  {
    "objectID": "common.scalers.html",
    "href": "common.scalers.html",
    "title": "TemporalNorm",
    "section": "",
    "text": "Figure 1. Illustration of temporal normalization (left), layer normalization (center) and batch normalization (right). The entries in green show the components used to compute the normalizing statistics.\n\n\n\n 1. Auxiliary Functions \n\n\nmasked_median\n\n masked_median (x, mask, dim=-1, keepdim=True)\n\nMasked Median\nCompute the median of tensor x along dim, ignoring values where mask is False. x and mask need to be broadcastable.\nParameters: x: torch.Tensor to compute median of along dim dimension. mask: torch Tensor bool with same shape as x, where x is valid and False where x should be masked. Mask should not be all False in any column of dimension dim to avoid NaNs from zero division. dim (int, optional): Dimension to take median of. Defaults to -1. keepdim (bool, optional): Keep dimension of x or not. Defaults to True.\nReturns: x_median: torch.Tensor with normalized values.\n\n\n\nmasked_mean\n\n masked_mean (x, mask, dim=-1, keepdim=True)\n\nMasked Mean\nCompute the mean of tensor x along dimension, ignoring values where mask is False. x and mask need to be broadcastable.\nParameters: x: torch.Tensor to compute mean of along dim dimension. mask: torch Tensor bool with same shape as x, where x is valid and False where x should be masked. Mask should not be all False in any column of dimension dim to avoid NaNs from zero division. dim (int, optional): Dimension to take mean of. Defaults to -1. keepdim (bool, optional): Keep dimension of x or not. Defaults to True.\nReturns: x_mean: torch.Tensor with normalized values.\n\n\n\n 2. Scalers \n\n\nminmax_scaler\n\n minmax_scaler (x, mask, eps=1e-06, dim=-1)\n\nMinMax Scaler\nStandardizes temporal features by ensuring its range dweels between [0,1] range. This transformation is often used as an alternative to the standard scaler. The scaled features are obtained as:\n\\[\\mathbf{z} = (\\mathbf{x}_{[B,T,C]}-\\mathrm{min}({\\mathbf{x}})_{[B,1,C]})/\n    (\\mathrm{max}({\\mathbf{x}})_{[B,1,C]}- \\mathrm{min}({\\mathbf{x}})_{[B,1,C]})\\]\nParameters: x: torch.Tensor input tensor. mask: torch Tensor bool, same dimension as x, indicates where x is valid and False where x should be masked. Mask should not be all False in any column of dimension dim to avoid NaNs from zero division. eps (float, optional): Small value to avoid division by zero. Defaults to 1e-6. dim (int, optional): Dimension over to compute min and max. Defaults to -1.\nReturns: z: torch.Tensor same shape as x, except scaled.\n\n\n\nminmax1_scaler\n\n minmax1_scaler (x, mask, eps=1e-06, dim=-1)\n\nMinMax1 Scaler\nStandardizes temporal features by ensuring its range dweels between [-1,1] range. This transformation is often used as an alternative to the standard scaler or classic Min Max Scaler. The scaled features are obtained as:\n\\[\\mathbf{z} = 2 (\\mathbf{x}_{[B,T,C]}-\\mathrm{min}({\\mathbf{x}})_{[B,1,C]})/ (\\mathrm{max}({\\mathbf{x}})_{[B,1,C]}- \\mathrm{min}({\\mathbf{x}})_{[B,1,C]})-1\\]\nParameters: x: torch.Tensor input tensor. mask: torch Tensor bool, same dimension as x, indicates where x is valid and False where x should be masked. Mask should not be all False in any column of dimension dim to avoid NaNs from zero division. eps (float, optional): Small value to avoid division by zero. Defaults to 1e-6. dim (int, optional): Dimension over to compute min and max. Defaults to -1.\nReturns: z: torch.Tensor same shape as x, except scaled.\n\n\n\nstd_scaler\n\n std_scaler (x, mask, dim=-1, eps=1e-06)\n\nStandard Scaler\nStandardizes features by removing the mean and scaling to unit variance along the dim dimension.\nFor example, for base_windows models, the scaled features are obtained as (with dim=1):\n\\[\\mathbf{z} = (\\mathbf{x}_{[B,T,C]}-\\bar{\\mathbf{x}}_{[B,1,C]})/\\hat{\\sigma}_{[B,1,C]}\\]\nParameters: x: torch.Tensor. mask: torch Tensor bool, same dimension as x, indicates where x is valid and False where x should be masked. Mask should not be all False in any column of dimension dim to avoid NaNs from zero division. eps (float, optional): Small value to avoid division by zero. Defaults to 1e-6. dim (int, optional): Dimension over to compute mean and std. Defaults to -1.\nReturns: z: torch.Tensor same shape as x, except scaled.\n\n\n\nrobust_scaler\n\n robust_scaler (x, mask, dim=-1, eps=1e-06)\n\nRobust Median Scaler\nStandardizes features by removing the median and scaling with the mean absolute deviation (mad) a robust estimator of variance. This scaler is particularly useful with noisy data where outliers can heavily influence the sample mean / variance in a negative way. In these scenarios the median and amd give better results.\nFor example, for base_windows models, the scaled features are obtained as (with dim=1):\n\\[\\mathbf{z} = (\\mathbf{x}_{[B,T,C]}-\\textrm{median}(\\mathbf{x})_{[B,1,C]})/\\textrm{mad}(\\mathbf{x})_{[B,1,C]}\\]\n\\[\\textrm{mad}(\\mathbf{x}) = \\frac{1}{N} \\sum_{}|\\mathbf{x} - \\mathrm{median}(x)|\\]\nParameters: x: torch.Tensor input tensor. mask: torch Tensor bool, same dimension as x, indicates where x is valid and False where x should be masked. Mask should not be all False in any column of dimension dim to avoid NaNs from zero division. eps (float, optional): Small value to avoid division by zero. Defaults to 1e-6. dim (int, optional): Dimension over to compute median and mad. Defaults to -1.\nReturns: z: torch.Tensor same shape as x, except scaled.\n\n\n\ninvariant_scaler\n\n invariant_scaler (x, mask, dim=-1, eps=1e-06)\n\nInvariant Median Scaler\nStandardizes features by removing the median and scaling with the mean absolute deviation (mad) a robust estimator of variance. Aditionally it complements the transformation with the arcsinh transformation.\nFor example, for base_windows models, the scaled features are obtained as (with dim=1):\n\\[\\mathbf{z} = (\\mathbf{x}_{[B,T,C]}-\\textrm{median}(\\mathbf{x})_{[B,1,C]})/\\textrm{mad}(\\mathbf{x})_{[B,1,C]}\\]\n\\[\\mathbf{z} = \\textrm{arcsinh}(\\mathbf{z})\\]\nParameters: x: torch.Tensor input tensor. mask: torch Tensor bool, same dimension as x, indicates where x is valid and False where x should be masked. Mask should not be all False in any column of dimension dim to avoid NaNs from zero division. eps (float, optional): Small value to avoid division by zero. Defaults to 1e-6. dim (int, optional): Dimension over to compute median and mad. Defaults to -1.\nReturns: z: torch.Tensor same shape as x, except scaled.\n\n\n\nidentity_scaler\n\n identity_scaler (x, mask, dim=-1, eps=1e-06)\n\nIdentity Scaler\nA placeholder identity scaler, that is argument insensitive.\nParameters: x: torch.Tensor input tensor. mask: torch Tensor bool, same dimension as x, indicates where x is valid and False where x should be masked. Mask should not be all False in any column of dimension dim to avoid NaNs from zero division. eps (float, optional): Small value to avoid division by zero. Defaults to 1e-6. dim (int, optional): Dimension over to compute median and mad. Defaults to -1.\nReturns: x: original torch.Tensor x.\n\n\n\n 3. TemporalNorm Module \n\n\nTemporalNorm\n\n TemporalNorm (scaler_type='robust', dim=-1, eps=1e-06)\n\nTemporal Normalization\nStandardization of the features is a common requirement for many machine learning estimators, and it is commonly achieved by removing the level and scaling its variance. The TemporalNorm module applies temporal normalization over the batch of inputs as defined by the type of scaler.\n\\[\\mathbf{z}_{[B,T,C]} = \\textrm{Scaler}(\\mathbf{x}_{[B,T,C]})\\]\nParameters: scaler_type: str, defines the type of scaler used by TemporalNorm. available [identity, standard, robust, minmax, minmax1, invariant]. dim (int, optional): Dimension over to compute scale and shift. Defaults to -1. eps (float, optional): Small value to avoid division by zero. Defaults to 1e-6.\n\n\n\nTemporalNorm.transform\n\n TemporalNorm.transform (x, mask)\n\nCenter and scale the data.\nParameters: x: torch.Tensor shape [batch, time, channels]. mask: torch Tensor bool, shape [batch, time] where x is valid and False where x should be masked. Mask should not be all False in any column of dimension dim to avoid NaNs from zero division.\nReturns: z: torch.Tensor same shape as x, except scaled.\n\n\n\nTemporalNorm.inverse_transform\n\n TemporalNorm.inverse_transform (z, x_shift=None, x_scale=None)\n\nScale back the data to the original representation.\nParameters: z: torch.Tensor shape [batch, time, channels], scaled.\nReturns: x: torch.Tensor original data.\n\n\n\n Example \n\nimport numpy as np\n\n\n# Declare synthetic batch to normalize\nx1 = 10**0 * np.arange(36)[:, None]\nx2 = 10**1 * np.arange(36)[:, None]\n\nnp_x = np.concatenate([x1, x2], axis=1)\nnp_x = np.repeat(np_x[None, :,:], repeats=2, axis=0)\nnp_x[0,:,:] = np_x[0,:,:] + 100\n\nnp_mask = np.ones(np_x.shape)\nnp_mask[:, -12:, :] = 0\n\nprint(f'x.shape [batch, time, features]={np_x.shape}')\nprint(f'mask.shape [batch, time, features]={np_mask.shape}')\n\n\n# Validate scalers\nx = 1.0*torch.tensor(np_x)\nmask = torch.tensor(np_mask)\nscaler = TemporalNorm(scaler_type='standard', dim=1)\nx_scaled = scaler.transform(x=x, mask=mask)\nx_recovered = scaler.inverse_transform(x_scaled)\n\nplt.plot(x[0,:,0], label='x1', color='#78ACA8')\nplt.plot(x[0,:,1], label='x2',  color='#E3A39A')\nplt.title('Before TemporalNorm')\nplt.xlabel('Time')\nplt.legend()\nplt.show()\n\nplt.plot(x_scaled[0,:,0], label='x1', color='#78ACA8')\nplt.plot(x_scaled[0,:,1]+0.1, label='x2+0.1', color='#E3A39A')\nplt.title(f'TemporalNorm \\'{scaler.scaler_type}\\' ')\nplt.xlabel('Time')\nplt.legend()\nplt.show()\n\nplt.plot(x_recovered[0,:,0], label='x1', color='#78ACA8')\nplt.plot(x_recovered[0,:,1], label='x2', color='#E3A39A')\nplt.title('Recovered')\nplt.xlabel('Time')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": " Models ",
    "section": "",
    "text": "source\n\n\n\n AutoRNN (h, config=None,\n          search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n          object at 0x7febfc2549d0>, num_samples=10, refit_with_val=False,\n          cpus=2, gpus=0, verbose=False)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengersDF as Y_df\n\n# Split train/test and declare time series dataset\nY_train_df = Y_df[Y_df.ds<='1959-12-31'] # 132 train\nY_test_df = Y_df[Y_df.ds>'1959-12-31']   # 12 test\ndataset, *_ = TimeSeriesDataset.from_df(Y_train_df)\n\n\n# Use your own config or AutoRNN.default_config\nconfig = dict(max_steps=10, input_size=-1, encoder_hidden_size=256)\nmodel = AutoRNN(h=12, config=config, num_samples=1, cpus=1)\n\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\n# Plotting predictions\nY_plot_df = Y_test_df[['unique_id', 'ds', 'y']].copy()\nY_plot_df['AutoRNN'] = y_hat\n\npd.concat([Y_train_df, Y_plot_df]).drop('unique_id', axis=1).set_index('ds').plot()\n\n\nsource\n\n\n\n\n AutoLSTM (h, config,\n           search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n           object at 0x7febfc53efd0>, num_samples=10,\n           refit_with_val=False, cpus=2, gpus=0, verbose=False)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs.\n\n# Use your own config or AutoLSTM.default_config\nconfig = dict(max_steps=10, input_size=-1)\nmodel = AutoLSTM(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoGRU (h, config,\n          search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n          object at 0x7febfc493310>, num_samples=10, refit_with_val=False,\n          cpus=2, gpus=0, verbose=False)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs.\n\n# Use your own config or AutoGRU.default_config\nconfig = dict(max_steps=10, input_size=-1)\nmodel = AutoGRU(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoTCN (h, config,\n          search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n          object at 0x7febfc1a3340>, num_samples=10, refit_with_val=False,\n          cpus=2, gpus=0, verbose=False)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs.\n\n# Use your own config or AutoTCN.default_config\nconfig = dict(max_steps=10, input_size=-1)\nmodel = AutoTCN(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoDilatedRNN (h, config,\n                 search_alg=<ray.tune.search.basic_variant.BasicVariantGen\n                 erator object at 0x7febfc227790>, num_samples=10,\n                 refit_with_val=False, cpus=2, gpus=0, verbose=False)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs.\n\n# Use your own config or AutoDilatedRNN.default_config\nconfig = dict(max_steps=10, input_size=-1)\nmodel = AutoDilatedRNN(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\n\n\n\n\nsource\n\n\n\n AutoMLP (h, config,\n          search_alg=<ray.tune.search.basic_variant.BasicVariantGenerator\n          object at 0x7febfc233df0>, num_samples=10, refit_with_val=False,\n          cpus=2, gpus=0, verbose=False)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs.\n\n# Use your own config or AutoMLP.default_config\nconfig = dict(max_steps=10, input_size=12)\nmodel = AutoMLP(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoNBEATS (h, config=None,\n             search_alg=<ray.tune.search.basic_variant.BasicVariantGenerat\n             or object at 0x7febfc1a25e0>, num_samples=10,\n             refit_with_val=False, cpus=2, gpus=0, verbose=False)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs.\n\n# Use your own config or AutoNBEATS.default_config\nconfig = dict(max_steps=10, input_size=12)\nmodel = AutoNBEATS(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n\nsource\n\n\n\n\n AutoNHITS (h, config=None,\n            search_alg=<ray.tune.search.basic_variant.BasicVariantGenerato\n            r object at 0x7febfc2043a0>, num_samples=10,\n            refit_with_val=False, cpus=2, gpus=0, verbose=False)\n\nBaseAuto\nClass for Automatic Hyperparameter Optimization, it builds on top of ray to give access to a wide variety of hyperparameter optimization tools ranging from classic grid search, to Bayesian optimization and HyperBand algorithm.\nThe validation loss to be optimized is defined by the config['loss'] dictionary value, the config also contains the rest of the hyperparameter search space.\nIt is important to note that the success of this hyperparameter optimization heavily relies on a strong correlation between the validation and test periods.\nParameters: cls_model: PyTorch/PyTorchLightning model, see neuralforecast.models collection here. h: int, forecast horizon. config: dict, dictionary with ray.tune defined search space. search_alg: ray.tune.search variant, BasicVariantGenerator, HyperOptSearch, DragonflySearch, TuneBOHB for details see tune.search. num_samples: int, number of hyperparameter optimization steps/samples. cpus: int, number of cpus to use during optimization, default all available. gpus: int, number of gpus to use during optimization, default all available. refit_wo_val: bool, number of gpus to use during optimization, default all available. verbose: bool, wether print partial outputs.\n\n# Use your own config or AutoNHITS.default_config\nconfig = dict(max_steps=1, input_size=12)\nmodel = AutoNHITS(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Example Data",
    "section": "",
    "text": "1. Synthetic Panel Data \n\nsource\n\ngenerate_series\n\n generate_series (n_series:int, freq:str='D', min_length:int=50,\n                  max_length:int=500, n_temporal_features:int=0,\n                  n_static_features:int=0, equal_ends:bool=False,\n                  seed:int=0)\n\nGenerate Synthetic Panel Series.\nGenerates n_series of frequency freq of different lengths in the interval [min_length, max_length]. If n_temporal_features > 0, then each serie gets temporal features with random values. If n_static_features > 0, then a static dataframe is returned along the temporal dataframe. If equal_ends == True then all series end at the same date.\nParameters: n_series: int, number of series for synthetic panel. min_length: int, minimal length of synthetic panel’s series. max_length: int, minimal length of synthetic panel’s series. n_temporal_features: int, default=0, number of temporal exogenous variables for synthetic panel’s series. n_static_features: int, default=0, number of static exogenous variables for synthetic panel’s series. equal_ends: bool, if True, series finish in the same date stamp ds. freq: str, frequency of the data, panda’s available frequencies.\nReturns: freq: pandas.DataFrame, synthetic panel with columns [unique_id, ds, y] and exogenous.\n\nfrom neuralforecast.utils import generate_series\n\nsynthetic_panel = generate_series(n_series=2)\nsynthetic_panel.groupby('unique_id').head(4)\n\n\n\n\n\n  \n    \n      \n      ds\n      y\n    \n    \n      unique_id\n      \n      \n    \n  \n  \n    \n      0\n      2000-01-01\n      0.357595\n    \n    \n      0\n      2000-01-02\n      1.301382\n    \n    \n      0\n      2000-01-03\n      2.272442\n    \n    \n      0\n      2000-01-04\n      3.211827\n    \n    \n      1\n      2000-01-01\n      5.399023\n    \n    \n      1\n      2000-01-02\n      6.092818\n    \n    \n      1\n      2000-01-03\n      0.476396\n    \n    \n      1\n      2000-01-04\n      1.343744\n    \n  \n\n\n\n\n\ntemporal_df, static_df = generate_series(n_series=1000, n_static_features=2,\n                                         n_temporal_features=4, equal_ends=False)\nstatic_df.head(2)\n\n\n\n\n2. AirPassengers Data \nThe classic Box & Jenkins airline data. Monthly totals of international airline passengers, 1949 to 1960.\nIt has been used as a reference on several forecasting libraries, since it is a series that shows clear trends and seasonalities it offers a nice opportunity to quickly showcase a model’s predictions performance.\n\nfrom neuralforecast.utils import AirPassengersDF\n\nAirPassengersDF.head(12)\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      1.0\n      1949-01-31\n      112.0\n    \n    \n      1\n      1.0\n      1949-02-28\n      118.0\n    \n    \n      2\n      1.0\n      1949-03-31\n      132.0\n    \n    \n      3\n      1.0\n      1949-04-30\n      129.0\n    \n    \n      4\n      1.0\n      1949-05-31\n      121.0\n    \n    \n      5\n      1.0\n      1949-06-30\n      135.0\n    \n    \n      6\n      1.0\n      1949-07-31\n      148.0\n    \n    \n      7\n      1.0\n      1949-08-31\n      148.0\n    \n    \n      8\n      1.0\n      1949-09-30\n      136.0\n    \n    \n      9\n      1.0\n      1949-10-31\n      119.0\n    \n    \n      10\n      1.0\n      1949-11-30\n      104.0\n    \n    \n      11\n      1.0\n      1949-12-31\n      118.0\n    \n  \n\n\n\n\n\n#We are going to plot the ARIMA predictions, and the prediction intervals.\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nplot_df = AirPassengersDF.set_index('ds')\n\nplot_df[['y']].plot(ax=ax, linewidth=2)\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n\n\nimport numpy as np\nimport pandas as pd\n\nn_static_features = 3\nn_series = 5\n\nstatic_features = np.random.uniform(low=0.0, high=1.0, \n                        size=(n_series, n_static_features))\nstatic_df = pd.DataFrame.from_records(static_features, \n                   columns = [f'static_{i}'for i in  range(n_static_features)])\nstatic_df['unique_id'] = np.arange(n_series)\n\n\nstatic_df\n\n\n\n3. Panel AirPassengers Data \nExtension to classic Box & Jenkins airline data. Monthly totals of international airline passengers, 1949 to 1960.\nIt includes two series with static, temporal and future exogenous variables, that can help to explore the performance of models like NBEATSx and TFT.\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nplot_df = AirPassengersPanel.set_index('ds')\n\nplot_df.groupby('unique_id')['y'].plot(legend=True)\nax.set_title('AirPassengers Panel Data', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(title='unique_id', prop={'size': 15})\nax.grid()\n\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nplot_df = AirPassengersPanel[AirPassengersPanel.unique_id=='Airline1'].set_index('ds')\n\nplot_df[['y', 'trend', 'y_[lag12]']].plot(ax=ax, linewidth=2)\nax.set_title('Box-Cox AirPassengers Data', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()"
  },
  {
    "objectID": "common.modules.html",
    "href": "common.modules.html",
    "title": "NN Modules",
    "section": "",
    "text": "Multi-Layer Perceptron\n\nsource\n\n\n\n MLP (in_features, out_features, activation, hidden_size, num_layers,\n      dropout)\n\nMulti-Layer Perceptron Class\nParameters: in_features: int, dimension of input. out_features: int, dimension of output. activation: str, activation function to use. hidden_size: int, dimension of hidden layers. num_layers: int, number of hidden layers. dropout: float, dropout rate."
  },
  {
    "objectID": "common.modules.html#temporal-convolutions",
    "href": "common.modules.html#temporal-convolutions",
    "title": "NN Modules",
    "section": "2. Temporal Convolutions",
    "text": "2. Temporal Convolutions\nFor long time in deep learning, sequence modelling was synonymous with recurrent networks, yet several papers have shown that simple convolutional architectures can outperform canonical recurrent networks like LSTMs by demonstrating longer effective memory.\nReferences -van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A. W., & Kavukcuoglu, K. (2016). Wavenet: A generative model for raw audio. Computing Research Repository, abs/1609.03499. URL: http://arxiv.org/abs/1609.03499. arXiv:1609.03499. -Shaojie Bai, Zico Kolter, Vladlen Koltun. (2018). An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling. Computing Research Repository, abs/1803.01271. URL: https://arxiv.org/abs/1803.01271.\n\n\nChomp1d\n\n Chomp1d (horizon)\n\nChomp1d\nReceives x input of dim [N,C,T], and trims it so that only ‘time available’ information is used. Used by one dimensional causal convolutions CausalConv1d.\nParameters: horizon: int, length of outsample values to skip.\n\n\n\nCausalConv1d\n\n CausalConv1d (in_channels, out_channels, kernel_size, padding, dilation,\n               activation, stride:int=1)\n\nCausal Convolution 1d\nReceives x input of dim [N,C_in,T], and computes a causal convolution in the time dimension. Skipping the H steps of the forecast horizon, through its dilation. Consider a batch of one element, the dilated convolution operation on the \\(t\\) time step is defined:\n\\(\\mathrm{Conv1D}(\\mathbf{x},\\mathbf{w})(t) = (\\mathbf{x}_{[*d]} \\mathbf{w})(t) = \\sum^{K}_{k=1} w_{k} \\mathbf{x}_{t-dk}\\)\nwhere \\(d\\) is the dilation factor, \\(K\\) is the kernel size, \\(t-dk\\) is the index of the considered past observation. The dilation effectively applies a filter with skip connections. If \\(d=1\\) one recovers a normal convolution.\nParameters: in_channels: int, dimension of x input’s initial channels. out_channels: int, dimension of x outputs’s channels. activation: str, identifying activations from PyTorch activations. select from ‘ReLU’,‘Softplus’,‘Tanh’,‘SELU’, ‘LeakyReLU’,‘PReLU’,‘Sigmoid’. padding: int, number of zero padding used to the left. kernel_size: int, convolution’s kernel size. dilation: int, dilation skip connections.\nReturns: x: tensor, torch tensor of dim [N,C_out,T] activation(conv1d(inputs, kernel) + bias). \n\n\n\nTemporalConvolutionEncoder\n\n TemporalConvolutionEncoder (in_channels, out_channels, kernel_size,\n                             dilations, activation:str='ReLU')\n\nTemporal Convolution Encoder\nReceives x input of dim [N,T,C_in], permutes it to [N,C_in,T] applies a deep stack of exponentially dilated causal convolutions. The exponentially increasing dilations of the convolutions allow for the creation of weighted averages of exponentially large long-term memory.\nParameters: in_channels: int, dimension of x input’s initial channels. out_channels: int, dimension of x outputs’s channels. kernel_size: int, size of the convolving kernel. dilations: int list, controls the temporal spacing between the kernel points. activation: str, identifying activations from PyTorch activations. select from ‘ReLU’,‘Softplus’,‘Tanh’,‘SELU’, ‘LeakyReLU’,‘PReLU’,‘Sigmoid’.\nReturns: x: tensor, torch tensor of dim [N,T,C_out]."
  },
  {
    "objectID": "models.lstm.html",
    "href": "models.lstm.html",
    "title": "LSTM",
    "section": "",
    "text": "The Long Short-Term Memory Recurrent Neural Network (LSTM), uses a multilayer LSTM encoder and an MLP decoder. It builds upon the LSTM-cell that improves the exploding and vanishing gradients of classic RNN’s. This network has been extensively used in sequential prediction tasks like language modeling, phonetic labeling, and forecasting. The predictions are obtained by transforming the hidden states into contexts \\(\\mathbf{c}_{[t+1:t+H]}\\), that are decoded and adapted into \\(\\mathbf{\\hat{y}}_{[t+1:t+H],[q]}\\) through MLPs.\n\\[\\begin{align}\n\\mathbf{h}_{t} &= \\textrm{LSTM}([\\mathbf{y}_{t},\\mathbf{x}^{(h)}_{t},\\mathbf{x}^{(s)}], \\mathbf{h}_{t-1})\\\\\n\\mathbf{c}_{[t+1:t+H]}&=\\textrm{Linear}([\\mathbf{h}_{t}, \\mathbf{x}^{(f)}_{[:t+H]}]) \\\\\n\\hat{y}_{\\tau,[q]}&=\\textrm{MLP}([\\mathbf{c}_{\\tau},\\mathbf{x}^{(f)}_{\\tau}])\n\\end{align}\\]\nwhere \\(\\mathbf{h}_{t}\\), is the hidden state for time \\(t\\), \\(\\mathbf{y}_{t}\\) is the input at time \\(t\\) and \\(\\mathbf{h}_{t-1}\\) is the hidden state of the previous layer at \\(t-1\\), \\(\\mathbf{x}^{(s)}\\) are static exogenous inputs, \\(\\mathbf{x}^{(h)}_{t}\\) historic exogenous, \\(\\mathbf{x}^{(f)}_{[:t+H]}\\) are future exogenous available at the time of the prediction.\nReferences-Jeffrey L. Elman (1990). “Finding Structure in Time”.-Haşim Sak, Andrew Senior, Françoise Beaufays (2014). “Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition.”\nsource"
  },
  {
    "objectID": "models.lstm.html#usage-example",
    "href": "models.lstm.html#usage-example",
    "title": "LSTM",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nfrom neuralforecast.tsdataset import TimeSeriesDataset, TimeSeriesLoader\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nfcst = NeuralForecast(\n    models=[LSTM(h=12, input_size=-1,\n                 #loss=MAE(),\n                 #loss=MQLoss(level=[80, 90]),\n                 loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n                 scaler_type='robust',\n                 encoder_n_layers=2,\n                 encoder_hidden_size=128,\n                 context_size=10,\n                 decoder_hidden_size=128,\n                 decoder_layers=2,\n                 max_epochs=200,\n                 futr_exog_list=None,\n                 hist_exog_list=['y_[lag12]'],\n                 stat_exog_list=['airline1'],\n                 )\n    ],\n    freq='M'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\ntry:\n    plt.plot(plot_df['ds'], plot_df['LSTM-median'], c='blue', label='median')\n    plt.fill_between(x=plot_df['ds'], \n                     y1=plot_df['LSTM-lo-90.0'], y2=plot_df['LSTM-hi-90.0'],\n                     alpha=0.4, label='level 90')\nexcept:\n    plt.plot(plot_df['ds'], plot_df['LSTM'], c='blue', label='median')\nplt.legend()\nplt.grid()\nplt.plot()"
  },
  {
    "objectID": "tsdataset.html",
    "href": "tsdataset.html",
    "title": "PyTorch Dataset/Loader",
    "section": "",
    "text": "source\n\nTimeSeriesLoader\n\n TimeSeriesLoader (dataset, **kwargs)\n\nTimeSeriesLoader DataLoader. Source code.\nSmall change to PyTorch’s Data loader. Combines a dataset and a sampler, and provides an iterable over the given dataset.\nThe class ~torch.utils.data.DataLoader supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning.\nParameters: batch_size: (int, optional): how many samples per batch to load (default: 1). shuffle: (bool, optional): set to True to have the data reshuffled at every epoch (default: False). sampler: (Sampler or Iterable, optional): defines the strategy to draw samples from the dataset. Can be any Iterable with __len__ implemented. If specified, shuffle must not be specified.\n\nsource\n\n\nTimeSeriesDataset\n\n TimeSeriesDataset (temporal, temporal_cols, indptr, max_size,\n                    static=None, static_cols=None, sorted=False)\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.\n\nsource\n\n\nTimeSeriesDataModule\n\n TimeSeriesDataModule (dataset:__main__.TimeSeriesDataset, batch_size=32,\n                       num_workers=0, drop_last=False)\n\nA DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is consistent data splits, data preparation and transforms across models.\nExample::\nclass MyDataModule(LightningDataModule):\n    def __init__(self):\n        super().__init__()\n    def prepare_data(self):\n        # download, split, etc...\n        # only called on 1 GPU/TPU in distributed\n    def setup(self, stage):\n        # make assignments here (val/train/test split)\n        # called on every process in DDP\n    def train_dataloader(self):\n        train_split = Dataset(...)\n        return DataLoader(train_split)\n    def val_dataloader(self):\n        val_split = Dataset(...)\n        return DataLoader(val_split)\n    def test_dataloader(self):\n        test_split = Dataset(...)\n        return DataLoader(test_split)\n    def teardown(self):\n        # clean up after fit or test\n        # called on every process in DDP\n\n# To test correct future_df wrangling of the `update_df` method\n# We are checking that we are able to recover the AirPassengers dataset\n# using the dataframe or splitting it into parts and initializing."
  },
  {
    "objectID": "models.tcn.html",
    "href": "models.tcn.html",
    "title": "TCN",
    "section": "",
    "text": "For long time in deep learning, sequence modelling was synonymous with recurrent networks, yet several papers have shown that simple convolutional architectures can outperform canonical recurrent networks like LSTMs by demonstrating longer effective memory. By skipping temporal connections the causal convolution filters can be applied to larger time spans while remaining computationally efficient.\nThe predictions are obtained by transforming the hidden states into contexts \\(\\mathbf{c}_{[t+1:t+H]}\\), that are decoded and adapted into \\(\\mathbf{\\hat{y}}_{[t+1:t+H],[q]}\\) through MLPs.\n\\[\\begin{align}\n\\mathbf{h}_{t} &= \\textrm{TCN}([\\mathbf{y}_{t},\\mathbf{x}^{(h)}_{t},\\mathbf{x}^{(s)}], \\mathbf{h}_{t-1})\\\\\n\\mathbf{c}_{[t+1:t+H]}&=\\textrm{Linear}([\\mathbf{h}_{t}, \\mathbf{x}^{(f)}_{[:t+H]}]) \\\\\n\\hat{y}_{\\tau,[q]}&=\\textrm{MLP}([\\mathbf{c}_{\\tau},\\mathbf{x}^{(f)}_{\\tau}])\n\\end{align}\\]\nwhere \\(\\mathbf{h}_{t}\\), is the hidden state for time \\(t\\), \\(\\mathbf{y}_{t}\\) is the input at time \\(t\\) and \\(\\mathbf{h}_{t-1}\\) is the hidden state of the previous layer at \\(t-1\\), \\(\\mathbf{x}^{(s)}\\) are static exogenous inputs, \\(\\mathbf{x}^{(h)}_{t}\\) historic exogenous, \\(\\mathbf{x}^{(f)}_{[:t+H]}\\) are future exogenous available at the time of the prediction.\nReferences -van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A. W., & Kavukcuoglu, K. (2016). Wavenet: A generative model for raw audio. Computing Research Repository, abs/1609.03499. URL: http://arxiv.org/abs/1609.03499. arXiv:1609.03499. -Shaojie Bai, Zico Kolter, Vladlen Koltun. (2018). An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling. Computing Research Repository, abs/1803.01271. URL: https://arxiv.org/abs/1803.01271.\nsource"
  },
  {
    "objectID": "models.tcn.html#usage-example",
    "href": "models.tcn.html#usage-example",
    "title": "TCN",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nfrom neuralforecast.tsdataset import TimeSeriesDataset, TimeSeriesLoader\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nfcst = NeuralForecast(\n    models=[TCN(h=12, input_size=-1,\n                #loss=MAE(),\n                #loss=MQLoss(level=[80, 90]),\n                loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n                learning_rate=5e-4,\n                kernel_size=2,\n                dilations=[1,2,4,8,16],\n                encoder_hidden_size=128,\n                context_size=10,\n                decoder_hidden_size=128,\n                decoder_layers=2,\n                max_epochs=500,\n                #scaler_type='robust',\n                scaler_type=None,\n                futr_exog_list=['y_[lag12]'],\n                hist_exog_list=None,\n                stat_exog_list=['airline1'],\n                )\n    ],\n    freq='M'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n\n# Plot quantile predictions\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\ntry:\n    plt.plot(plot_df['ds'], plot_df['TCN-median'], c='blue', label='median')\n    plt.fill_between(x=plot_df['ds'], \n                     y1=plot_df['TCN-lo-90.0'], y2=plot_df['TCN-hi-90.0'],\n                     alpha=0.4, label='level 90')\nexcept:\n    plt.plot(plot_df['ds'], plot_df['TCN'], c='blue', label='median')\nplt.legend()\nplt.grid()\nplt.plot()"
  },
  {
    "objectID": "models.nbeatsx.html",
    "href": "models.nbeatsx.html",
    "title": "NBEATSx",
    "section": "",
    "text": "The Neural Basis Expansion Analysis (NBEATS) is an MLP-based deep neural architecture with backward and forward residual links. The network has two variants: (1) in its interpretable configuration, NBEATS sequentially projects the signal into polynomials and harmonic basis to learn trend and seasonality components; (2) in its generic configuration, it substitutes the polynomial and harmonic basis for identity basis and larger network’s depth. The Neural Basis Expansion Analysis with Exogenous (NBEATSx), incorporates projections to exogenous temporal variables available at the time of the prediction. This method proved state-of-the-art performance on the M3, M4, and Tourism Competition datasets, improving accuracy by 3% over the ESRNN M4 competition winner. For Electricity Price Forecasting tasks NBEATSx model improved accuracy by 20% and 5% over ESRNN and NBEATS, and 5% on task-specialized architectures.References-Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio (2019). “N-BEATS: Neural basis expansion analysis for interpretable time series forecasting”.-Kin G. Olivares, Cristian Challu, Grzegorz Marcjasz, Rafał Weron, Artur Dubrawski (2021). “Neural basis expansion analysis with exogenous variables: Forecasting electricity prices with NBEATSx”.\nsource"
  },
  {
    "objectID": "models.nbeatsx.html#usage-example",
    "href": "models.nbeatsx.html#usage-example",
    "title": "NBEATSx",
    "section": "Usage Example",
    "text": "Usage Example\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.losses.pytorch import MQLoss, DistributionLoss\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = NBEATSx(h=12, input_size=24,\n                #loss=MQLoss(level=[80, 90]),\n                loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n                scaler_type='robust',\n                stat_exog_list=['airline1'],\n                futr_exog_list=['trend'],\n                max_epochs=200)\n\nfcst = NeuralForecast(\n    models=[model],\n    freq='M'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot quantile predictions\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['NBEATSx-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'], \n                 y1=plot_df['NBEATSx-lo-90.0'], y2=plot_df['NBEATSx-hi-90.0'],\n                 alpha=0.4, label='level 90.0')\nplt.grid()\nplt.legend()\nplt.plot()"
  }
]